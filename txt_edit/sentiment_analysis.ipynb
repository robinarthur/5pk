{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hierf√ºr brauche ich den Wortschatz der Uni Leipzig bzw. SentiWS und Beispieltexte als Corpus\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "class Splitter(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def split(self, text):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        input format: a paragraph of text\n",
    "        \n",
    "        output format: a list of lists of words.\n",
    "        \n",
    "           e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        \n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentence]\n",
    "        \n",
    "        return tokenized_sentences\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class POSTagger(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def pos_tag(self, sentences):\n",
    "        \n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "          e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "        \n",
    "          e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']),('sentence', 'sentence', ['NN'])],\n",
    "                 [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']),('one', 'one',\n",
    "                 ['CARD'])]]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        \n",
    "        #adapt format\n",
    "        \n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        \n",
    "        return pos\n",
    "    \n",
    "    \n",
    "\n",
    "class DictionaryTagger(object):\n",
    "    \n",
    "    def __init__(self, dictionary_paths):\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "                  \n",
    "    \n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=false):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right               \n",
    "        \"\"\"\n",
    "        \n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) # avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
