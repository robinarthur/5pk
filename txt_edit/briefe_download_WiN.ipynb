{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next try with some manual tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re, os\n",
    "\n",
    "def remove_duplicates(values):\n",
    "    output = []\n",
    "    seen = set()\n",
    "    for value in values:\n",
    "        # If value has not been encountered yet,\n",
    "        # ... add it to both list and set.\n",
    "        if value not in seen:\n",
    "            output.append(value)\n",
    "            seen.add(value)\n",
    "    return output\n",
    "\n",
    "\n",
    "def make_soup(url):\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    url = urlopen(req)\n",
    "    content = url.read()\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_the_letter(url):\n",
    "    soup = make_soup(url)\n",
    "    text = soup.find(\"div\", {\"id\": \"main\"})\n",
    "    return text.text\n",
    "\n",
    "def create_dir(name):\n",
    "    try:\n",
    "        if not os.path.exists(name):\n",
    "            os.makedirs(name)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' + name)\n",
    "\n",
    "# open the file with the year links:\n",
    "with open('./input/Briefe/years.txt', 'r') as f:\n",
    "    years = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "letter_links = []\n",
    "\n",
    "# open link for link, scrape through the site and save all of links\n",
    "for year in years:\n",
    "    req = Request(str(year))\n",
    "    html_page = urlopen(req)\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    for link in soup.findAll('a'):\n",
    "        letter_links.append(link.get('href'))\n",
    "        \n",
    "print(len(letter_links))\n",
    "# some cleaning\n",
    "# first remove the duplicates\n",
    "letter_links = remove_duplicates(letter_links)\n",
    "# remove the impressum, etc\n",
    "things = ['#main',\n",
    "          '../../../../index.htm',\n",
    "          '../../../index.htm',\n",
    "          '../../index.htm',\n",
    "          '../index.htm',\n",
    "          '../../../../legal/impressum.htm',\n",
    "          '../../../../legal/disclaimer.htm',\n",
    "          '../../../../legal/datenschutz.htm',\n",
    "          '../../../../legal/copyright.htm']\n",
    "\n",
    "for thing in things:\n",
    "    while thing in letter_links: letter_links.remove(thing) \n",
    "\n",
    "# remove the mailto-links\n",
    "letter_links = [x for x in letter_links if not x.startswith('mailto:')]\n",
    "# remove the letters i still have downloaded manually\n",
    "letter_links = [x for x in letter_links if not x.startswith('1772')]\n",
    "letter_links = [x for x in letter_links if not x.startswith('1778')]\n",
    "letter_links = [x for x in letter_links if not x.startswith('1780')]\n",
    "letter_links = [x for x in letter_links if not x.startswith('1781')]\n",
    "\n",
    "temp_path = './input/Briefe/to_process/'\n",
    "\n",
    "base_start = 'http://www.wissen-im-netz.info/literatur/schiller/briefe/'\n",
    "next_url = base_start + str(i) + '/'\n",
    "\n",
    "# make a list of all the scraped years\n",
    "# after that: \n",
    "# 1. build the complete link for the download\n",
    "# 2. open a temp file and store the downloaded site in it\n",
    "# 3. process the file to get the text of the letter\n",
    "# 4. make the filename\n",
    "# 5. check if the filename exists - if yes - open it and store the 'new' letter in it\n",
    "# 6. if not save it\n",
    "n = 0\n",
    "year_list = []\n",
    "for letter_link in letter_links:\n",
    "    yl = str(letter_link)\n",
    "    yl = yl[:4]\n",
    "    if yl not in year_list:\n",
    "        year_list.append(yl)\n",
    "    # 1. build the complete link of the letter\n",
    "    url = base_start + str(yl) + '/' + str(letter_link)\n",
    "    # 2., 3.,\n",
    "    tempfile = temp_path + 'temp.txt'\n",
    "    soup = get_the_letter(url)\n",
    "    print(url)\n",
    "    \n",
    "    text = str(soup)\n",
    "    \n",
    "    with open(tempfile, 'w') as temp:\n",
    "        temp.write(text)\n",
    "        \n",
    "    # 4. make the filename\n",
    "    f_name = letter_link[letter_link.find(\"-\") + 1:]\n",
    "    f_name = f_name[:-4]\n",
    "    print(f_name)\n",
    "    \n",
    "    dir_path = './input/Briefe/to_process/' + str(yl) + '/'\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    \n",
    "    # open the tempfile\n",
    "    with open(tempfile, 'r') as f:\n",
    "        text_lines = f.readlines()\n",
    "        \n",
    "    # remove first 3 lines (' ', 'Friedrich Schiller', 'Friedrich Schiller an Friedrich von Hoven')\n",
    "    text_lines = text_lines[3:]\n",
    "    \n",
    "    text = ''\n",
    "    for line in text_lines:\n",
    "        text += line\n",
    "    \n",
    "    #write the letter into its own textfile\n",
    "    l_path = './input/Briefe/to_process/'+ str(yl) + '/' + str(f_name) + '.txt'\n",
    "    \n",
    "    # 5., 6. check if the file exists and save it: \n",
    "    mode = 'a+' if os.path.exists(l_path) else 'w+'\n",
    "    with open(l_path, mode) as f:\n",
    "        f.write(text)\n",
    "\n",
    "    ## Try to delete the temp file ##\n",
    "    try:\n",
    "        os.remove(tempfile)\n",
    "    except OSError as e:  ## if failed, report it back to the user ##\n",
    "        print (\"Error: %s - %s.\" % (e.filename, e.strerror))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy and process all the letters into the input txt folder and all the data into schiller.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "source_path = './input/Briefe/to_process/'\n",
    "dest_path = './input/Briefe/done/'\n",
    "tempfile = dest_path + 'tempfile.txt'\n",
    "\n",
    "# folders are from year 1782 to 1805\n",
    "\n",
    "number = 197\n",
    "l = []\n",
    "header = ['#', 'Name', 'Type', 'Year']\n",
    "l.append(header)\n",
    "\n",
    "for y in range(1782,1806):\n",
    "    \n",
    "    year = y\n",
    "    for root, dirs, titles, in os.walk(source_path + str(year), topdown=False):\n",
    "        for title in titles:\n",
    "            \n",
    "            text = ''\n",
    "\n",
    "            # open file after file\n",
    "            with open(source_path + str(year) + '/' + title, 'r') as f:\n",
    "                f_read_lines = f.readlines()\n",
    "                \n",
    "            for line in f_read_lines:\n",
    "                text += line\n",
    "            \n",
    "            with open(dest_path + '0' + str(number) + '.txt', 'w+') as temp:\n",
    "                temp.write(text)\n",
    "                   \n",
    "            title = title[:-4]\n",
    "            title = title.replace('-', ' ')\n",
    "            book = [str(number), str(title), 'B', str(y)]\n",
    "            l.append(book)\n",
    "            number += 1\n",
    "#print(book)\n",
    "#print(l)\n",
    "#print(number)\n",
    "\n",
    "df = pd.DataFrame(l[1:],columns=l[0]).set_index('#')\n",
    "df.to_csv('./input/Briefe/schiller_letters.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
