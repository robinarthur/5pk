{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import zipfile\n",
    "import os.path\n",
    "import glob\n",
    "from chardet.universaldetector import UniversalDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unzip(source_filename, dest_dir):\n",
    "    \"\"\"this function unzips all the files from the epub file\n",
    "        the following functions should only need the textfiles\"\"\"\n",
    "    with zipfile.ZipFile(source_filename) as zf:\n",
    "        for member in zf.infolist():\n",
    "            # Path traversal defense copied from\n",
    "            # http://hg.python.org/cpython/file/tip/Lib/http/server.py#l789\n",
    "            words = member.filename.split('/')\n",
    "            path = dest_dir\n",
    "            for word in words[:-1]:\n",
    "                while True:\n",
    "                    drive, word = os.path.splitdrive(word)\n",
    "                    head, word = os.path.split(word)\n",
    "                    if not drive:\n",
    "                        print(\"no drive\")\n",
    "                        break\n",
    "                if word in (os.curdir, os.pardir, ''):\n",
    "                    continue\n",
    "                path = os.path.join(path, word)\n",
    "            zf.extract(member, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_txt(dir):\n",
    "    for path in glob.glob(dir + '/*.html'):\n",
    "        with open(path) as markup:\n",
    "            soup = BeautifulSoup(markup.read(),\"html5lib\" )\n",
    "        with open(\"strip_\" + path, \"w\") as f:\n",
    "            f.write(soup.get_text().encode('ISO-8859-2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect(file):\n",
    "    detector = UniversalDetector()\n",
    "    for filename in glob.glob(file):\n",
    "        print(filename.ljust(60),detector.reset())\n",
    "        for line in file(filename, 'rb'):\n",
    "            detector.feed(line)\n",
    "            if detector.done:\n",
    "                break\n",
    "    \n",
    "    detector.close()\n",
    "    print(detector.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'schiller.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e7690d334923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"schiller.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"html5lib\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ISO-8859-2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'schiller.txt'"
     ]
    }
   ],
   "source": [
    "def convert(file, coding):\n",
    "    f = open(file)\n",
    "    \n",
    "    \n",
    "    \n",
    "f = open(\"schiller.txt\")\n",
    "soup = BeautifulSoup(f.read(),\"html5lib\" )\n",
    "g = soup.get_text().encode('ISO-8859-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "       index                                           Sentence  Book_ID\n",
      "0          0  \\n\\n\\n\\ne-artnow\\n\\n\\n\\r\\ndiv.sgc-1 {margin-le...        0\n",
      "1          1  Moromesk kann der Ort sein, wo die Desna vorbe...        0\n",
      "2          2  Man sieht Tschernigow jenseits der Desna links...        0\n",
      "3          3          Demetrius führt die Armee über die Desna.        0\n",
      "4          4  Man sieht die Türme von Tschernigow, noch weit...        0\n",
      "5          5  Demetrius erinnert sich, daß er als ein entlau...        0\n",
      "6          6  Die Armee kommt aus einem Wald, der ihr die Au...        0\n",
      "7          7  Odowalsky befiehlt den Obersten, die Armee lin...        0\n",
      "8          8  Aussenden von Manifesten und Agenten in die Pl...        0\n",
      "9          9                    Zustand der russischen Grenzen.        0\n",
      "10        10  Man erfährt diesen durch die Zurückkunft eines...        0\n",
      "11        11     Gesandtschaft der Kosaken, wann fällt sie vor?        0\n",
      "12        12                                     Das gute Omen.        0\n",
      "13        13                          Disposition des Feldzugs.        0\n",
      "14        14                           Man geht über die Desna.        0\n",
      "15        15     Ein Teil des Heers trennt sich von dem andern.        0\n",
      "16        16  Demetrius an der russischen Grenze\\n\\n\\nDie Sz...        0\n",
      "17        17  Es ist eine unermeßliche Ferne, ein prächtiger...        0\n",
      "18        18  Man hört kriegerische Trommeln, und die Offizi...        0\n",
      "19        19  Vorher wird noch gesagt, daß die Armee unten w...        0\n",
      "20        20  Er prallt beim Anblick der freien Landschaft m...        0\n",
      "21        21  »Ha welch ein Anblick« – Großer Zar, du siehst...        0\n",
      "22        22                               »Ist das die Grenze?        0\n",
      "23        23  – Ist das der Dnjepr, der sich majestätisch du...        0\n",
      "24        24       Und was du siehst, ist deines Reiches Boden!        0\n",
      "25        25  – Hier diese Säule trägt schon russisch Wappen...        0\n",
      "26        26                            »Welch heitrer Anblick!        0\n",
      "27        27  Welche schöne Auen!«\\nDer Lenz hat sie mit sei...        0\n",
      "28        28         Lob des Bodens, der Fülle des Korns trägt.        0\n",
      "29        29  »Das Auge schwimmt hin im unermeßlichen Gesich...        0\n",
      "...      ...                                                ...      ...\n",
      "42103   2101                                              Fort.       12\n",
      "42104   2102     Hedwigeilt herein:\\r\\n        Wo bist du Tell?       12\n",
      "42105   2103                                   Der Vater kommt!       12\n",
      "42106   2104  Es nahn in frohem Zug\\r\\n        Die Eidgenoss...       12\n",
      "42107   2105         Ich darf nicht weilen bei den Glücklichen.       12\n",
      "42108   2106                  Tell:\\r\\n        Geh liebes Weib.       12\n",
      "42109   2107  Erfrische diesen Mann,\\r\\n        Belad ihn re...       12\n",
      "42110   2108                                              Eile!       12\n",
      "42111   2109                                          Sie nahn.       12\n",
      "42112   2110                     Hedwig:\\r\\n        Wer ist es?       12\n",
      "42113   2111                    Tell:\\r\\n        Forsche nicht!       12\n",
      "42114   2112  Und wenn er geht, so wende deine Augen,\\r\\n   ...       12\n",
      "42115   2113  Parricida geht auf den Tell zu mit einer rasch...       12\n",
      "42116   2114  Wenn beide zu verschiedenen Seiten abgegangen,...       12\n",
      "42117   2115  Andere kommen über einen hohen Steg, der über ...       12\n",
      "42118   2116  Walther Fürst mit den beiden Knaben, Melchtal ...       12\n",
      "42119   2117                     Alle:\\r\\n        Es lebe Tell!       12\n",
      "42120   2118                       der Schütz und der Erretter!       12\n",
      "42121   2119  Indem sich die vordersten um den Tell drängen ...       12\n",
      "42122   2120  Die Musik vom Berge begleitet diese stumme Szene.       12\n",
      "42123   2121  Wenn sie geendigt, tritt Berta in die Mitte de...       12\n",
      "42124   2122                       Berta:\\r\\n        Landleute!       12\n",
      "42125   2123                                       Eidgenossen!       12\n",
      "42126   2124  Nehmt mich auf\\r\\n        In euern Bund, die e...       12\n",
      "42127   2125  In eure tapfre Hand leg ich mein Recht,\\r\\n   ...       12\n",
      "42128   2126  Landleute:\\r\\n        Das wollen wir mit Gut u...       12\n",
      "42129   2127                          Berta:\\r\\n        Wohlan!       12\n",
      "42130   2128  So reich ich diesem Jüngling meine Rechte,\\r\\n...       12\n",
      "42131   2129  Rudenz:\\r\\n        Und frei erklär ich alle me...       12\n",
      "42132   2130  Indem die Musik von neuem rasch einfällt, fäll...       12\n",
      "\n",
      "[42133 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# coding: latin1\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#!pip3 install -U spacy\n",
    "#!python -m spacy download de\n",
    "#import spacy\n",
    "import codecs\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlite3 import Error\n",
    "from urllib import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "corpus_root = '/home/user/Dokumente/Github/5pk/txt_edit'\n",
    "\n",
    "\n",
    "# The books are splitted into several files. First i have to group the \n",
    "# files for every book and put them into one textstring\n",
    "\n",
    "# html_folder\n",
    "loc = \"/html\"\n",
    "\n",
    "#book1 - Demetrius\n",
    "raw_book_1_1 = loc + '/' + 'Demetrius.html'\n",
    "raw_book_1 = [raw_book_1_1]\n",
    "\n",
    "#book2 - Der versöhnte Menschenfeind\n",
    "raw_book_2_1 = loc + '/' + 'DerVers_0.html'\n",
    "raw_book_2_2 = loc + '/' + 'DerVers_1.html'\n",
    "raw_book_2 = [raw_book_2_1, raw_book_2_2]\n",
    "\n",
    "#book3 - Die Braut von Messina\n",
    "raw_book_3_1 = loc + '/' + 'DieBraut_0.html'\n",
    "raw_book_3_2 = loc + '/' + 'DieBraut_1.html'\n",
    "raw_book_3_3 = loc + '/' + 'DieBraut_2.html'\n",
    "raw_book_3_4 = loc + '/' + 'DieBraut_3.html'\n",
    "raw_book_3_5 = loc + '/' + 'DieBraut_4.html'\n",
    "raw_book_3_6 = loc + '/' + 'DieBraut_5.html'\n",
    "raw_book_3_7 = loc + '/' + 'DieBraut_6.html'\n",
    "raw_book_3 = [raw_book_3_1, raw_book_3_2, raw_book_3_3, raw_book_3_4, \n",
    "              raw_book_3_5, raw_book_3_6, raw_book_3_7\n",
    "             ]\n",
    "\n",
    "#book4 - Die Huldigung der Künste - 1 - DieHuldi.html\n",
    "raw_book_4_1 = loc + '/' + 'DieHuldi.html'\n",
    "raw_book_4 = [raw_book_4_1]\n",
    "\n",
    "#book5 - Die Jungfrau von Orleans - 10 - DieJungfrau_00.html\n",
    "raw_book_5_1 = loc + '/' + 'DieJungfrau_00.html'\n",
    "raw_book_5_2 = loc + '/' + 'DieJungfrau_01.html'\n",
    "raw_book_5_3 = loc + '/' + 'DieJungfrau_02.html'\n",
    "raw_book_5_4 = loc + '/' + 'DieJungfrau_03.html'\n",
    "raw_book_5_5 = loc + '/' + 'DieJungfrau_04.html'\n",
    "raw_book_5_6 = loc + '/' + 'DieJungfrau_05.html'\n",
    "raw_book_5_7 = loc + '/' + 'DieJungfrau_06.html'\n",
    "raw_book_5_8 = loc + '/' + 'DieJungfrau_07.html'\n",
    "raw_book_5_9 = loc + '/' + 'DieJungfrau_08.html'\n",
    "raw_book_5_10 = loc + '/' + 'DieJungfrau_09.html'\n",
    "raw_book_5 = [raw_book_5_1, raw_book_5_2, raw_book_5_3, raw_book_5_4,\n",
    "              raw_book_5_5, raw_book_5_6, raw_book_5_7, raw_book_5_8,\n",
    "              raw_book_5_9, raw_book_5_10]\n",
    "\n",
    "#book6 - Die Räuber - 9 - DieRauber_0.html\n",
    "raw_book_6_1 = loc + '/' + 'DieRauber_0.html'\n",
    "raw_book_6_2 = loc + '/' + 'DieRauber_1.html'\n",
    "raw_book_6_3 = loc + '/' + 'DieRauber_2.html'\n",
    "raw_book_6_4 = loc + '/' + 'DieRauber_3.html'\n",
    "raw_book_6_5 = loc + '/' + 'DieRauber_4.html'\n",
    "raw_book_6_6 = loc + '/' + 'DieRauber_5.html'\n",
    "raw_book_6_7 = loc + '/' + 'DieRauber_6.html'\n",
    "raw_book_6_8 = loc + '/' + 'DieRauber_7.html'\n",
    "raw_book_6_9 = loc + '/' + 'DieRauber_8.html'\n",
    "raw_book_6 = [raw_book_6_1, raw_book_6_2, raw_book_6_3, raw_book_6_4,\n",
    "              raw_book_6_5, raw_book_6_6, raw_book_6_7, raw_book_6_8,\n",
    "              raw_book_6_9]\n",
    "\n",
    "#book7 - Die Verschwörung des fiesco zu Genua 13 - DieVersch_00.html\n",
    "raw_book_7_1 = loc + '/' + 'DieVersch_00.html'\n",
    "raw_book_7_2 = loc + '/' + 'DieVersch_01.html'\n",
    "raw_book_7_3 = loc + '/' + 'DieVersch_02.html'\n",
    "raw_book_7_4 = loc + '/' + 'DieVersch_03.html'\n",
    "raw_book_7_5 = loc + '/' + 'DieVersch_04.html'\n",
    "raw_book_7_6 = loc + '/' + 'DieVersch_05.html'\n",
    "raw_book_7_7 = loc + '/' + 'DieVersch_06.html'\n",
    "raw_book_7_8 = loc + '/' + 'DieVersch_07.html'\n",
    "raw_book_7_9 = loc + '/' + 'DieVersch_08.html'\n",
    "raw_book_7_10 = loc + '/' + 'DieVersch_09.html'\n",
    "raw_book_7_11 = loc + '/' + 'DieVersch_10.html'\n",
    "raw_book_7_12 = loc + '/' + 'DieVersch_11.html'\n",
    "raw_book_7_13 = loc + '/' + 'DieVersch_12.html'\n",
    "raw_book_7 = [raw_book_7_1, raw_book_7_2, raw_book_7_3, raw_book_7_4,\n",
    "              raw_book_7_5, raw_book_7_6, raw_book_7_7, raw_book_7_8,\n",
    "              raw_book_7_9, raw_book_7_10, raw_book_7_11, raw_book_7_12,\n",
    "              raw_book_7_13]\n",
    "\n",
    "#book8 - Don Carlos - 10 - DonCarlos_0.html\n",
    "raw_book_8_1 = loc + '/' + 'DonCarlos_0.html'\n",
    "raw_book_8_2 = loc + '/' + 'DonCarlos_1.html'\n",
    "raw_book_8_3 = loc + '/' + 'DonCarlos_2.html'\n",
    "raw_book_8_4 = loc + '/' + 'DonCarlos_3.html'\n",
    "raw_book_8_5 = loc + '/' + 'DonCarlos_4.html'\n",
    "raw_book_8_6 = loc + '/' + 'DonCarlos_5.html'\n",
    "raw_book_8_7 = loc + '/' + 'DonCarlos_6.html'\n",
    "raw_book_8_8 = loc + '/' + 'DonCarlos_7.html'\n",
    "raw_book_8_9 = loc + '/' + 'DonCarlos_8.html'\n",
    "raw_book_8_10 = loc + '/' + 'DonCarlos_9.html'\n",
    "raw_book_8 = [raw_book_8_1, raw_book_8_2, raw_book_8_3, raw_book_8_4,\n",
    "              raw_book_8_5, raw_book_8_6, raw_book_8_7, raw_book_8_8,\n",
    "              raw_book_8_9]\n",
    "\n",
    "#book9 - Kabale und Liebe 7 - KabaleUnd_0.html\n",
    "raw_book_9_1 = loc + '/' + 'KabaleUnd_0.html'\n",
    "raw_book_9_2 = loc + '/' + 'KabaleUnd_1.html'\n",
    "raw_book_9_3 = loc + '/' + 'KabaleUnd_2.html'\n",
    "raw_book_9_4 = loc + '/' + 'KabaleUnd_3.html'\n",
    "raw_book_9_5 = loc + '/' + 'KabaleUnd_4.html'\n",
    "raw_book_9_6 = loc + '/' + 'KabaleUnd_5.html'\n",
    "raw_book_9_7 = loc + '/' + 'KabaleUnd_6.html'\n",
    "raw_book_9 = [raw_book_9_1, raw_book_9_2, raw_book_9_3, raw_book_9_4,\n",
    "              raw_book_9_5, raw_book_9_6, raw_book_9_7]\n",
    "\n",
    "#book10- Maria Stuart - 7 - MariaStuart_0.html\n",
    "raw_book_10_1 = loc + '/' + 'MariaStuart_0.html'\n",
    "raw_book_10_2 = loc + '/' + 'MariaStuart_1.html'\n",
    "raw_book_10_3 = loc + '/' + 'MariaStuart_2.html'\n",
    "raw_book_10_4 = loc + '/' + 'MariaStuart_3.html'\n",
    "raw_book_10_5 = loc + '/' + 'MariaStuart_4.html'\n",
    "raw_book_10_6 = loc + '/' + 'MariaStuart_5.html'\n",
    "raw_book_10_7 = loc + '/' + 'MariaStuart_6.html'\n",
    "raw_book_10 = [raw_book_10_1, raw_book_10_2, raw_book_10_3, raw_book_10_4,\n",
    "               raw_book_10_5, raw_book_10_6, raw_book_10_7]\n",
    "\n",
    "#book11- Semele\n",
    "raw_book_11_1 = loc + '/' + 'Semele_0.html'\n",
    "raw_book_11_2 = loc + '/' + 'Semele_1.html'\n",
    "raw_book_11_3 = loc + '/' + 'Semele_2.html'\n",
    "raw_book_11 = [raw_book_11_1, raw_book_11_2, raw_book_11_3]\n",
    "\n",
    "#book12- Wallenstein\n",
    "raw_book_12_1 = loc + '/' + 'Wallenstein_0.html'\n",
    "raw_book_12_2 = loc + '/' + 'Wallenstein_1.html'\n",
    "raw_book_12_3 = loc + '/' + 'Wallenstein_2.html'\n",
    "raw_book_12_4 = loc + '/' + 'Wallenstein_3.html'\n",
    "raw_book_12_5 = loc + '/' + 'Wallenstein_4.html'\n",
    "raw_book_12_6 = loc + '/' + 'Wallenstein_5.html'\n",
    "raw_book_12_7 = loc + '/' + 'Wallenstein_6.html'\n",
    "raw_book_12_8 = loc + '/' + 'Wallenstein_7.html'\n",
    "raw_book_12_9 = loc + '/' + 'Wallenstein_8.html'\n",
    "raw_book_12_10 = loc + '/' + 'Wallenstein_9.html'\n",
    "raw_book_12_11 = loc + '/' + 'Wallenstein_10.html'\n",
    "raw_book_12_12 = loc + '/' + 'Wallenstein_11.html'\n",
    "raw_book_12_13 = loc + '/' + 'Wallenstein_12.html'\n",
    "raw_book_12_14 = loc + '/' + 'Wallenstein_13.html'\n",
    "raw_book_12_15 = loc + '/' + 'Wallenstein_14.html'\n",
    "raw_book_12_16 = loc + '/' + 'Wallenstein_15.html'\n",
    "raw_book_12_17 = loc + '/' + 'Wallenstein_16.html'\n",
    "raw_book_12_18 = loc + '/' + 'Wallenstein_17.html'\n",
    "raw_book_12_19 = loc + '/' + 'Wallenstein_18.html'\n",
    "raw_book_12_20 = loc + '/' + 'Wallenstein_19.html'\n",
    "raw_book_12_21 = loc + '/' + 'Wallenstein_20.html'\n",
    "raw_book_12 = [raw_book_12_1, raw_book_12_2, raw_book_12_3, raw_book_12_4,\n",
    "               raw_book_12_5, raw_book_12_6, raw_book_12_7, raw_book_12_8,\n",
    "               raw_book_12_9, raw_book_12_10, raw_book_12_11, raw_book_12_12,\n",
    "               raw_book_12_13, raw_book_12_14, raw_book_12_15, raw_book_12_16,\n",
    "               raw_book_12_17, raw_book_12_18, raw_book_12_19, raw_book_12_20,\n",
    "               raw_book_12_21\n",
    "              ]\n",
    "\n",
    "#book13- Wilhelm Tell\n",
    "raw_book_13_1 = loc + '/' + 'WilhelmTell_0.html'\n",
    "raw_book_13_2 = loc + '/' + 'WilhelmTell_1.html'\n",
    "raw_book_13_3 = loc + '/' + 'WilhelmTell_3.html'\n",
    "raw_book_13_4 = loc + '/' + 'WilhelmTell_4.html'\n",
    "raw_book_13_5 = loc + '/' + 'WilhelmTell_5.html'\n",
    "raw_book_13_6 = loc + '/' + 'WilhelmTell_6.html'\n",
    "raw_book_13_7 = loc + '/' + 'WilhelmTell_7.html'\n",
    "raw_book_13 = [raw_book_13_1, raw_book_13_2, raw_book_13_3, raw_book_13_4,\n",
    "               raw_book_13_5, raw_book_13_6, raw_book_13_7\n",
    "              ]\n",
    "\n",
    "raw_books_schiller = [(raw_book_1,1),\n",
    "                      (raw_book_2,2),\n",
    "                      (raw_book_3,7),\n",
    "                      (raw_book_4,1),\n",
    "                      (raw_book_5,10),\n",
    "                      (raw_book_6,9),\n",
    "                      (raw_book_7,13),\n",
    "                      (raw_book_8,9),\n",
    "                      (raw_book_9,7),\n",
    "                      (raw_book_10,7),\n",
    "                      (raw_book_11,3),\n",
    "                      (raw_book_12,21),\n",
    "                      (raw_book_13,7)\n",
    "                     ]\n",
    "\n",
    "print(type(raw_books_schiller))\n",
    "\n",
    "\n",
    "# NLTK's default German stopwords\n",
    "default_stopwords = set(nltk.corpus.stopwords.words('german'))\n",
    "custom_stopwords = set((u'–', u'dass', u'mehr', u'0000ff', u'000ff', u'0em', u'1.2', u'1em', u'2em', u'a.sgc-2',\n",
    "                        u'center', u'color', u'div.sgc-1', u'div.sgc-5', u'div.sgc-toc-level-1',\n",
    "                        u'div.sgc-toc-level-2', u'div.sgc-toc-level-3', u'div.sgc-toc-level-4',\n",
    "                        u'div.sgc-toc-level-5', u'div.sgc-toc-level-6', u'div.sgc-toc-title', u'document',\n",
    "                        u'e-artnow', u'font-face', u'font-size', u'h1.sgc-4', u'h2.sgc-1', u'h2.sgc-3', \n",
    "                        u'inhaltsverzeichnis', u'line-height', u'umargin-bottom', u'margin-left', u'margin:20',\n",
    "                        u' ', u'text-align', u'text-decoration'\n",
    "                       ))\n",
    "\n",
    "all_stopwords = default_stopwords | custom_stopwords\n",
    "\n",
    "def stick_the_books_together(book, number):\n",
    "    \"\"\" This function get all the different html books and put them to one book text/unicode string.\n",
    "    \n",
    "        input: \n",
    "        - book: text string\n",
    "        the path of the given book\n",
    "        \n",
    "        - number: integer\n",
    "        how many books are published\n",
    "        \n",
    "        \n",
    "        output:\n",
    "        result: text of all the books\n",
    "        all the books    \n",
    "    \"\"\"\n",
    "    all_raw_text = ''\n",
    "    \n",
    "    for i in range(number):\n",
    "        #print(i)\n",
    "        path = corpus_root + book[i]\n",
    "        html = open(path).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        raw = soup.get_text()\n",
    "        all_raw_text = all_raw_text + raw\n",
    "    \n",
    "    return(all_raw_text)\n",
    "\n",
    "\n",
    "def create_connection(database):\n",
    "    \"\"\" create a database connection to the SQLite database \n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(database)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None \n",
    "\n",
    "\n",
    "def create_table(conn, create_table_sql):\n",
    "    \"\"\" create a table from the create_table_sql statement\n",
    "        param conn: Connection object\n",
    "        param create_table_sql: a CREATE TABLE statement\n",
    "        return:\"\"\"\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(create_table_sql)\n",
    "    except Error as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "def create_the_book_db():\n",
    "    \n",
    "    database = './db/5pk.db'\n",
    "    \n",
    "    sql_create_tbl_Authors = \"\"\" CREATE TABLE IF NOT EXISTS tbl_Authors (\n",
    "                                    Author_ID integer NOT NULL Primary Key,\n",
    "                                    Author_NAME text NOT NULL);\"\"\"\n",
    "    \n",
    "    sql_create_tbl_Books = \"\"\" CREATE TABLE IF NOT EXISTS tbl_Books (\n",
    "                                    Book_ID integer NOT NULL Primary Key,\n",
    "                                    Book_NAME text NOT NULL,\n",
    "                                    Author_ID integer NOT NULL,\n",
    "                                    Year integer NOT NULL,\n",
    "                                    FOREIGN KEY (Author_ID) REFERENCES\n",
    "                                    tbl_Authors(Author_ID));\"\"\"\n",
    "    \n",
    "    sql_create_tbl_Sentences = \"\"\" CREATE TABLE IF NOT EXISTS tbl_Sentences (\n",
    "                                    Sentence_ID integer Primary Key,\n",
    "                                    Book_ID integer NOT NULL,\n",
    "                                    Sentence TEXT NOT NULL,\n",
    "                                    FOREIGN KEY (Book_ID) REFERENCES\n",
    "                                    tbl_Books(Book_ID));\"\"\"\n",
    "\n",
    "    \n",
    "    sql_create_ind_Authors_Author_ID = \"\"\" CREATE INDEX ind_Authors_Author_ID\n",
    "                                            ON tbl_Authors(Author_ID);\"\"\"\n",
    "    \n",
    "    sql_create_ind_Books_Book_ID = \"\"\" CREATE INDEX ind_Books_Book_ID\n",
    "                                        ON tbl_Books(Book_ID);\"\"\"\n",
    "    \n",
    "    sql_create_ind_Sentences_Sentence_ID = \"\"\" CREATE INDEX ind_Sentences_Sentence_ID \n",
    "                                                ON tbl_Sentences(Sentence_ID);\"\"\"\n",
    "    \n",
    "    # Connecting to the database file\n",
    "    conn = create_connection('./db/5pk.db')\n",
    "\n",
    "    if conn is not None:\n",
    "        #create Authors table\n",
    "        create_table(conn, sql_create_tbl_Authors)\n",
    "        #create Books table\n",
    "        create_table(conn, sql_create_tbl_Books)\n",
    "        #create Sentences\n",
    "        create_table(conn, sql_create_tbl_Sentences)\n",
    "        \n",
    "        #create the Indexes\n",
    "        create_table(conn, sql_create_ind_Authors_Author_ID)\n",
    "        create_table(conn, sql_create_ind_Books_Book_ID)\n",
    "        create_table(conn, sql_create_ind_Sentences_Sentence_ID)\n",
    "    else:\n",
    "        print(\"ERROR! cannot create the database connection.\")\n",
    "\n",
    "\n",
    "def create_author(conn, author):\n",
    "    \"\"\"\n",
    "    Create a new Author\n",
    "    :param conn:\n",
    "    :param author:\n",
    "    :return: Author_ID\n",
    "    \"\"\"\n",
    "    sql = '''INSERT INTO tbl_Authors(Author_NAME, )'''\n",
    "        \n",
    "def create_book(conn, book):\n",
    "    \"\"\"\n",
    "    Create a new book into the tbl_Books\n",
    "    :param conn:\n",
    "    :param book:\n",
    "    :return: book id\n",
    "    \"\"\"\n",
    "    sql = '''INSERT INTO tbl_Books(Book_NAME, Author_ID, Year)\n",
    "            VALUES(?,?,?) '''\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, book)\n",
    "    \n",
    "    return cur.lastrowid\n",
    "\n",
    "\n",
    "def create_sentences(conn, sentence):\n",
    "    \"\"\"\n",
    "    Create a new Sentence\n",
    "    :param conn:\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sql = ''' INSERT INTO tbl_Sentences(Book_ID, Sentence)\n",
    "            Values(?,?) '''\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, sentence)\n",
    "    return cur.lastrowid\n",
    "        \n",
    "\n",
    "def select_all_sentences(conn):\n",
    "    \"\"\"\n",
    "    Query all rows in the tbl_Sentences table\n",
    "    :param conn: the connection object\n",
    "    :return: print every row\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT * FROM tbl_Sentences\")\n",
    "    \n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    for row in rows:\n",
    "        print(row)\n",
    "        \n",
    "        \n",
    "def select_sentences_by_sentence_id(conn, Book_ID):\n",
    "    \"\"\"\n",
    "    Query sentence by ID\n",
    "    :param conn: the connection object\n",
    "    :param Book_ID: \n",
    "    :return: print every row\n",
    "    \"\"\"\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELCET * FROM tbl_Sentences WHERE Book_ID=?\", (Book_ID,))\n",
    "    \n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    for row in rows:\n",
    "        print(row)\n",
    "    \n",
    "def fill_the_db_with_books(list_of_books):\n",
    "    db = './db/5pk.db'    \n",
    "    conn = create_connection(db)\n",
    "    conn.text_factory = str\n",
    "    #INSERT INTO tbl_Books(Book_NAME, Author_ID, Year)\n",
    "    df = pd.DataFrame(list_of_books, columns=['Book_NAME', 'Author_ID', 'Year'])\n",
    "    #put the book list into the db\n",
    "    df.to_sql(\"tbl_Books\", conn, if_exists=\"append\")\n",
    "    \n",
    "    #check if everything is really in the db table\n",
    "    test = pd.read_sql_query(\"select * from tbl_Books;\", conn)\n",
    "    return test\n",
    "\n",
    "def fill_the_db_with_sentence(df_of_sentences):\n",
    "    db = './db/5pk.db'\n",
    "    conn = create_connection(db)\n",
    "    conn.text_factory = str\n",
    "    #INSERT INTO tbl_Sentence(Sentence, Book_ID)\n",
    "    #df = pd.DataFrame(list_of_sentences, columns=['Sentence', 'Book_ID'])\n",
    "    df = df_of_sentences\n",
    "    #put the sentences from a book into the db\n",
    "    df.to_sql(\"tbl_Sentences\", conn, if_exists=\"append\")\n",
    "    \n",
    "    #check if everything is really in the db table\n",
    "    test = pd.read_sql_query(\"select * from tbl_Sentences;\", conn)\n",
    "    return test\n",
    "\n",
    "def fill_the_db_with_authors(list_of_authors):\n",
    "    db = './db/5pk.db'\n",
    "    conn = create_connection(db)\n",
    "    conn.text_factory = str\n",
    "    #INSERT INTO tbl_Authors(Author_NAME)\n",
    "    df = pd.DataFrame(list_of_authors, columns=['Author_NAME'])\n",
    "    #put the author list into the db Authors\n",
    "    df.to_sql(\"tbl_Authors\", conn, if_exists=\"append\")\n",
    "    \n",
    "    #check if everything is really in the db author\n",
    "    test = pd.read_sql_query(\"select * from tbl_Authors;\", conn)\n",
    "    return test\n",
    "\n",
    "# book_list for the tbl_Books\n",
    "# this is the first lis in december 2017\n",
    "book_list = [('Demetrius', 0, 1805),\n",
    "               ('Der versöhnte Menschenfeind', 0, 1790),\n",
    "               ('Die Braut von Messina', 0, 1803),\n",
    "               ('Die Huldigung der Künste', 0, 1804),\n",
    "               ('Die Jungfrau von Orleans', 0, 1801),\n",
    "               ('Die Räuber', 0, 1781),\n",
    "               ('Die Verschwörung des Fiesco zu Genua', 0, 1784),\n",
    "               ('Don Carlos', 0, 1787),\n",
    "               ('Kabale und Liebe', 0, 1783),\n",
    "               ('Maria Stuart', 0, 1800),\n",
    "               ('Semele', 0, 1782),\n",
    "               ('Wallenstein', 0, 1799),\n",
    "               ('Wilhelm Tell', 0, 1804)              \n",
    "              ]\n",
    "\n",
    "author_list = [('Friedrich Schiller')]\n",
    "\n",
    "\n",
    "\n",
    "#create the book DB in the beginning, only once\n",
    "#create_the_book_db()\n",
    "#fill_the_db_with_authors(author_list)\n",
    "#fill_the_db_with_books(book_list)\n",
    "\n",
    "\n",
    "def process_every_text_and_put_the_sentences_into_a_df(list_of_books):\n",
    "    \"\"\"\n",
    "    :param list_of_books: [(\"name of book\", number of books)]: \n",
    "    [(raw_book_1,1),(raw_book_2,2)]\n",
    "    \n",
    "    :param conn: connection to the Database\n",
    "    \n",
    "    1. the list of book has to processed with the stick_the_books_together function\n",
    "    2. this raw text get the number i from the book list and this is the Book_ID number for the tbl_Books\n",
    "    \"\"\"\n",
    "    \n",
    "    Book_ID = list_of_books[0][0]\n",
    "    print(Book_ID)\n",
    "\n",
    "#process_every_text_and_put_the_sentences_into_a_df(raw_books_schiller)\n",
    "\n",
    "def create_sentences_list(text):\n",
    "    s =  sent_tokenize(text)\n",
    "    #remove the \\n stuff\n",
    "    \n",
    "    return s\n",
    "\n",
    "#sentences = create_sentences_list(wallenstein_all_raw)\n",
    "\n",
    "\n",
    "#for i in sentences:\n",
    "#    print(i)\n",
    "\n",
    "#test = nltk.Text(sentences)\n",
    "#print(len(test))\n",
    "\n",
    "\"\"\"\n",
    "following code is taken from\n",
    "https://github.com/AlliedToasters/nlp/blob/master/4.4.2_Challenge_0.ipynb\n",
    "\"\"\"\n",
    "\n",
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\"\"\"    \n",
    "# Load and clean the data.\n",
    "wallenstein_all_raw = stick_the_books_together(raw_book_12, 21)\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "wallenstein = re.sub(r'Chapter \\d+', '', wallenstein_all_raw)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "wallenstein = text_cleaner(wallenstein)\n",
    "\n",
    "\n",
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "#persuasion_doc = nlp(persuasion)\n",
    "\n",
    "\n",
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "#ersuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents)# + persuasion_sents)\n",
    "sentences.head()\n",
    "\"\"\"\n",
    "\n",
    "def text_analysis(all_raw_text, start = int, end = int):\n",
    "    \"\"\" This function get all_raw_text - maybe from stick_the_books_together() - \n",
    "    and did some analysis to return some wordlists\n",
    "    \n",
    "    the start and end characters are her to sort some type out at the beginning and the end of the book\n",
    "    \n",
    "    input:\n",
    "    - all_raw_text: unicode\n",
    "    - start character: int\n",
    "    - end character: int    \n",
    "    \"\"\"\n",
    "    \n",
    "    raw = all_raw_text[start:]\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    \n",
    "    text = nltk.Text(tokens)\n",
    "    \n",
    "    # Remove single-character tokens (mostly punctuation)\n",
    "    text = [word for word in text if len(word) > 1]\n",
    "    \n",
    "    # Remove numbers\n",
    "    words = [word for word in text if not word.isnumeric()]\n",
    "    \n",
    "    # lowercase all words (default_stopwords are lowercase too)\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in all_stopwords]\n",
    "    \n",
    "    # return the processed word list\n",
    "    return words\n",
    "\n",
    "#test = text_analysis(wallenstein_all_raw, 400 ,70)\n",
    "#test = sorted(set(test))\n",
    "#for i in test:\n",
    "#    print(i)\n",
    "\n",
    "\n",
    "#this could be a function to \n",
    "# first - stick the books together\n",
    "# second - create sentences\n",
    "# third - made a DataFrame\n",
    "# fourth - append another column (Book_ID)\n",
    "# result is a Sentence-DF ready for sql input\n",
    "\n",
    "def stick_the_books_create_sentences_create_df(Book_ID, book_files, number_of_books):\n",
    "    text = stick_the_books_together(book_files, number_of_books)\n",
    "    text = create_sentences_list(text)\n",
    "    \n",
    "    text = pd.DataFrame(text, columns = [\"Sentence\"])\n",
    "    text['Book_ID'] = Book_ID\n",
    "    return text\n",
    "\n",
    "\n",
    "def put_all_book_sentences_into_the_db():\n",
    "    Demetrius = stick_the_books_create_sentences_create_df(0, raw_book_1, 1)\n",
    "    DerversoehnteMenschenfeind = stick_the_books_create_sentences_create_df(1, raw_book_2, 2)\n",
    "    DieBrautvonMessina = stick_the_books_create_sentences_create_df(2, raw_book_3, 7)\n",
    "    DieHuldigungderKuenste = stick_the_books_create_sentences_create_df(3, raw_book_4, 1)\n",
    "    DieJungfrauvonOrleans = stick_the_books_create_sentences_create_df(4, raw_book_5, 10)\n",
    "    DieRaeuber = stick_the_books_create_sentences_create_df(5, raw_book_6, 9)\n",
    "    DieVerschwoerungdesFiescozuGenua = stick_the_books_create_sentences_create_df(6, raw_book_7, 13)\n",
    "    DonCarlos = stick_the_books_create_sentences_create_df(7, raw_book_8, 9)\n",
    "    KabaleundLiebe = stick_the_books_create_sentences_create_df(8, raw_book_9, 7)\n",
    "    MariaStuart = stick_the_books_create_sentences_create_df(9, raw_book_10, 7)\n",
    "    Semele = stick_the_books_create_sentences_create_df(10, raw_book_11, 3)\n",
    "    Wallenstein = stick_the_books_create_sentences_create_df(11, raw_book_12, 21)\n",
    "    WilhelmTell = stick_the_books_create_sentences_create_df(12, raw_book_13, 7)\n",
    "    \n",
    "    ## Fill the Dataframes into the DB\n",
    "    sentence_db = fill_the_db_with_sentence(Demetrius)\n",
    "    sentence_db = fill_the_db_with_sentence(DerversoehnteMenschenfeind)\n",
    "    sentence_db = fill_the_db_with_sentence(DieBrautvonMessina)\n",
    "    sentence_db = fill_the_db_with_sentence(DieHuldigungderKuenste)\n",
    "    sentence_db = fill_the_db_with_sentence(DieJungfrauvonOrleans)\n",
    "    sentence_db = fill_the_db_with_sentence(DieRaeuber)\n",
    "    sentence_db = fill_the_db_with_sentence(DieVerschwoerungdesFiescozuGenua)\n",
    "    sentence_db = fill_the_db_with_sentence(DonCarlos)\n",
    "    sentence_db = fill_the_db_with_sentence(KabaleundLiebe)\n",
    "    sentence_db = fill_the_db_with_sentence(MariaStuart)\n",
    "    sentence_db = fill_the_db_with_sentence(Semele)\n",
    "    sentence_db = fill_the_db_with_sentence(Wallenstein)\n",
    "    sentence_db = fill_the_db_with_sentence(WilhelmTell)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(sentence_db)\n",
    "\n",
    "put_all_book_sentences_into_the_db()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### have to check the following\n",
    "\n",
    "\n",
    "#Download the POS Tagger first\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "#<ConllCorpusReader in u'/home/user/Dokumente/Github/5pk/txt_edit'>\n",
    "root = '.'\n",
    "fileid = 'tiger.16012013.conll09'\n",
    "columntypes = ['ignore', 'words', 'ignore', 'ignore', 'pos']\n",
    "corp = nltk.corpus.ConllCorpusReader(root, fileid, columntypes, encoding='utf8')\n",
    "\n",
    "\n",
    "#load the tiger corpus to train the pos tagger later\n",
    "#corp = nltk.corpus.ConllCorpusReader('.', 'tiger_release_aug07.corrected.16012013.conll09',\n",
    "#                                     ['ignore', 'words', 'ignore', 'ignore', 'pos'],\n",
    "#                                     encoding='utf-8')\n",
    "\"\"\"\n",
    "print(corp)\n",
    "print(type(corp))\n",
    "print(corp.tagged_sents())\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "# explicitly make list, then LazySequence will traverse all items\n",
    "#https://stackoverflow.com/questions/39622121/nltk-perceptron-tagger-typeerror-lazysubsequence-object-does-not-support-ite\n",
    "tagged_sentences = [corp.tagged_sents()]\n",
    "i = math.floor(len(tagged_sentences)*0.2)\n",
    "testing_sentences = tagged_sentences[0:int(i)]\n",
    "training_sentences = tagged_sentences[int(i):]\n",
    "\n",
    "perceptron_tagger = nltk.tag.perceptron.PerceptronTagger(load=False)\n",
    "print(\"testtesttest\")\n",
    "print(training_sentences)\n",
    "print(len(training_sentences))\n",
    "perceptron_tagger.train(training_sentences)\n",
    "\n",
    "print(type(perceptron_tagger))\n",
    "\"\"\"\n",
    "\"\"\"the solution:\n",
    " https://stackoverflow.com/questions/39622121/nltk-perceptron-tagger-typeerror-lazysubsequence-object-does-not-support-ite\n",
    "\n",
    "Solution\n",
    "\n",
    "To work around the error, just explicitly create a list of the sentences from the nltk.corpus.brown package then random can shuffle the data properly.\n",
    "\n",
    "<code>\n",
    "    import nltk,math\n",
    "     explicitly make list, then LazySequence will traverse all items\n",
    "    tagged_sentences = [sentence for sentence in nltk.corpus.brown.tagged_sents(categories='news',tagset='universal')]\n",
    "    i = math.floor(len(tagged_sentences)*0.2)\n",
    "    testing_sentences = tagged_sentences[0:i]\n",
    "    training_sentences = tagged_sentences[i:]\n",
    "    perceptron_tagger = nltk.tag.perceptron.PerceptronTagger(load=False)\n",
    "    perceptron_tagger.train(training_sentences)\n",
    "\n",
    "</code>\n",
    "no error, yea!\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# set a split size: use 90% for training, 10% for testing\n",
    "split_perc = 0.1\n",
    "split_size = int(len(tagged_sents) * split_perc)\n",
    "train_sents, test_sents = tagged_sents[split_size:], tagged_sents[:split_size]\n",
    "\n",
    "\"\"\"\n",
    "#do a pos tagging for the test text\n",
    "#test = nltk.pos_tag(test)\n",
    "#print(test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for book in raw_books_schiller:\n",
    "    path = corpus_root + book[0]\n",
    "    html = open(path).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    raw = soup.get_text()\n",
    "    print(\"########### jetzt kommt der Type\")\n",
    "    print(type(raw))\n",
    "    raw = raw[237:]\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    \n",
    "    text = nltk.Text(tokens)\n",
    "\n",
    "    # Remove single-character tokens (mostly punctuation)\n",
    "    text = [word for word in text if len(word) > 1]\n",
    "    \n",
    "    # Remove numbers\n",
    "    words = [word for word in text if not word.isnumeric()]\n",
    "    \n",
    "    # Lowercase all words (default_stopwords are lowercase too)\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in all_stopwords]\n",
    "    \n",
    "    # Calculate frequency distribution\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    \n",
    "    # Output top 20 words\n",
    "    for word, frequency in fdist.most_common(20):\n",
    "        print(u'{};{}'.format(word, frequency))\n",
    "\n",
    "    \n",
    "#    fdist1 = FreqDist(words)\n",
    "#    fdist1.most_common(50)\n",
    "#    \n",
    "#    vocab = sorted(set(words))\n",
    "#    \n",
    "#    for word in vocab:\n",
    "#        print(word)\n",
    "\"\"\"   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "i = 0\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    print(filename)\n",
    "    f = open(path + '/' + filename)\n",
    "    soup = BeautifulSoup(f.read(), \"html5lib\")\n",
    "    i += 1\n",
    "    for i in soup.findAll(u\"\\u2018\"):\n",
    "        if i.text == u\"\\2018\" or u\"\\2019\" in i.text:\n",
    "            i.string = \"'\"\n",
    "    for i in soup.findAll(u\"\\u2019\"):\n",
    "        if i.text == u\"\\2018\" or u\"\\2019\" in i.text:\n",
    "            i.string = \"'\"\n",
    "    g = soup.get_text().encode('utf-8')\n",
    "    print(g)\n",
    "    \n",
    "    \n",
    "#for file in glob.glob('*.html'):\n",
    " #   detect()\n",
    " \n",
    " \"\"\"\n",
    "\n",
    "\"\"\"for book in raw_books_schiller:\n",
    "    print(book)\n",
    "    pcr = PlaintextCorpusReader(corpus_root + book[0],'.*')\n",
    "    pcr.fileids()\n",
    "    print(wordlist)\n",
    "    #num_chars = len(wordlist)\n",
    "    test = gutenberg.raw(wordlist)\n",
    "    num_words = gutenberg.words(wordlist)\n",
    "    num_sents = gutenberg.sents(wordlist)\n",
    "\n",
    "\n",
    "for book in raw_books_schiller:\n",
    "    num_chars = len(gutenberg.raw(book))\n",
    "    num_words = len(gutenberg.words(book))\n",
    "    num_sents = len(gutenberg.sents(book))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
