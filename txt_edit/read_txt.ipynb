{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import zipfile\n",
    "import os.path\n",
    "import glob\n",
    "from chardet.universaldetector import UniversalDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unzip(source_filename, dest_dir):\n",
    "    \"\"\"this function unzips all the files from the epub file\n",
    "        the following functions should only need the textfiles\"\"\"\n",
    "    with zipfile.ZipFile(source_filename) as zf:\n",
    "        for member in zf.infolist():\n",
    "            # Path traversal defense copied from\n",
    "            # http://hg.python.org/cpython/file/tip/Lib/http/server.py#l789\n",
    "            words = member.filename.split('/')\n",
    "            path = dest_dir\n",
    "            for word in words[:-1]:\n",
    "                while True:\n",
    "                    drive, word = os.path.splitdrive(word)\n",
    "                    head, word = os.path.split(word)\n",
    "                    if not drive:\n",
    "                        print(\"no drive\")\n",
    "                        break\n",
    "                if word in (os.curdir, os.pardir, ''):\n",
    "                    continue\n",
    "                path = os.path.join(path, word)\n",
    "            zf.extract(member, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_txt(dir):\n",
    "    for path in glob.glob(dir + '/*.html'):\n",
    "        with open(path) as markup:\n",
    "            soup = BeautifulSoup(markup.read(),\"html5lib\" )\n",
    "        with open(\"strip_\" + path, \"w\") as f:\n",
    "            f.write(soup.get_text().encode('ISO-8859-2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect(file):\n",
    "    detector = UniversalDetector()\n",
    "    for filename in glob.glob(file):\n",
    "        print(filename.ljust(60),detector.reset())\n",
    "        for line in file(filename, 'rb'):\n",
    "            detector.feed(line)\n",
    "            if detector.done:\n",
    "                break\n",
    "    \n",
    "    detector.close()\n",
    "    print(detector.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'schiller.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e7690d334923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"schiller.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"html5lib\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ISO-8859-2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'schiller.txt'"
     ]
    }
   ],
   "source": [
    "def convert(file, coding):\n",
    "    f = open(file)\n",
    "    \n",
    "    \n",
    "    \n",
    "f = open(\"schiller.txt\")\n",
    "soup = BeautifulSoup(f.read(),\"html5lib\" )\n",
    "g = soup.get_text().encode('ISO-8859-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "<ConllCorpusReader in u'/home/user/Dokumente/Github/5pk/txt_edit'>\n",
      "<class 'nltk.corpus.reader.conll.ConllCorpusReader'>\n",
      "[[(u'``', u'$('), (u'Ross', u'NE'), (u'Perot', u'NE'), (u'w\\xe4re', u'VAFIN'), (u'vielleicht', u'ADV'), (u'ein', u'ART'), (u'pr\\xe4chtiger', u'ADJA'), (u'Diktator', u'NN'), (u\"''\", u'$(')], [(u'Konzernchefs', u'NN'), (u'lehnen', u'VVFIN'), (u'den', u'ART'), (u'Milliard\\xe4r', u'NN'), (u'als', u'APPR'), (u'US-Pr\\xe4sidenten', u'NN'), (u'ab', u'PTKVZ'), (u'/', u'$(')], ...]\n",
      "testtesttest\n",
      "[[[(u'``', u'$('), (u'Ross', u'NE'), (u'Perot', u'NE'), (u'w\\xe4re', u'VAFIN'), (u'vielleicht', u'ADV'), (u'ein', u'ART'), (u'pr\\xe4chtiger', u'ADJA'), (u'Diktator', u'NN'), (u\"''\", u'$(')], [(u'Konzernchefs', u'NN'), (u'lehnen', u'VVFIN'), (u'den', u'ART'), (u'Milliard\\xe4r', u'NN'), (u'als', u'APPR'), (u'US-Pr\\xe4sidenten', u'NN'), (u'ab', u'PTKVZ'), (u'/', u'$(')], ...]]\n",
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8bdbccc1df99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m \u001b[0mperceptron_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperceptron_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, save_loc, nr_iter)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# to be populated by self._make_tagdict...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_tagdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miter_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnr_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36m_make_tagdict\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "# coding: latin1\n",
    "import os\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "from urllib import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "corpus_root = '/home/user/Dokumente/Github/5pk/txt_edit'\n",
    "\n",
    "\n",
    "\n",
    "# The books are splitted into several files. First i have to group the \n",
    "# files for every book and put them into one textstring\n",
    "\n",
    "# html_folder\n",
    "loc = \"/html\"\n",
    "\n",
    "#book1 - Demetrius\n",
    "raw_book_1_1 = loc + '/' + 'Demetrius.html'\n",
    "raw_book_1 = [raw_book_1_1]\n",
    "\n",
    "\n",
    "#book2 - Der versöhnte Menschenfeind\n",
    "raw_book_2_1 = loc + '/' + 'DerVers_0.html'\n",
    "raw_book_2_2 = loc + '/' + 'DerVers_1.html'\n",
    "raw_book_2 = [raw_book_2_1, raw_book_2_2]\n",
    "\n",
    "\n",
    "#book3 - Die Braut von Messina\n",
    "raw_book_3_1 = loc + '/' + 'DieBraut_0.html'\n",
    "raw_book_3_2 = loc + '/' + 'DieBraut_1.html'\n",
    "raw_book_3_3 = loc + '/' + 'DieBraut_2.html'\n",
    "raw_book_3_4 = loc + '/' + 'DieBraut_3.html'\n",
    "raw_book_3_5 = loc + '/' + 'DieBraut_4.html'\n",
    "raw_book_3_6 = loc + '/' + 'DieBraut_5.html'\n",
    "raw_book_3_7 = loc + '/' + 'DieBraut_6.html'\n",
    "raw_book_3 = [raw_book_3_1, raw_book_3_2, raw_book_3_3, raw_book_3_4, \n",
    "              raw_book_3_5, raw_book_3_6, raw_book_3_7\n",
    "             ]\n",
    "\n",
    "\n",
    "#book4 - Die Huldigung der Künste - 1 - DieHuldi.html\n",
    "raw_book_4 = [loc + '/' + 'DieHuldi.html']\n",
    "\n",
    "\n",
    "#book5 - Die Jungfrau von Orleans - 10 - DieJungfrau_00.html\n",
    "raw_book_5_1 = loc + '/' + 'DieJungfrau_00.html'\n",
    "raw_book_5_2 = loc + '/' + 'DieJungfrau_01.html'\n",
    "raw_book_5_3 = loc + '/' + 'DieJungfrau_02.html'\n",
    "raw_book_5_4 = loc + '/' + 'DieJungfrau_03.html'\n",
    "raw_book_5_5 = loc + '/' + 'DieJungfrau_04.html'\n",
    "raw_book_5_6 = loc + '/' + 'DieJungfrau_05.html'\n",
    "raw_book_5_7 = loc + '/' + 'DieJungfrau_06.html'\n",
    "raw_book_5_8 = loc + '/' + 'DieJungfrau_07.html'\n",
    "raw_book_5_9 = loc + '/' + 'DieJungfrau_08.html'\n",
    "raw_book_5_10 = loc + '/' + 'DieJungfrau_09.html'\n",
    "raw_book_5 = [raw_book_5_1, raw_book_5_2, raw_book_5_3, raw_book_5_4,\n",
    "              raw_book_5_5, raw_book_5_6, raw_book_5_7, raw_book_5_8,\n",
    "              raw_book_5_9, raw_book_5_10]\n",
    "\n",
    "\n",
    "#book6 - Die Räuber - 9 - DieRauber_0.html\n",
    "raw_book_6_1 = loc + '/' + 'DieRauber_0.html'\n",
    "raw_book_6_2 = loc + '/' + 'DieRauber_1.html'\n",
    "raw_book_6_3 = loc + '/' + 'DieRauber_2.html'\n",
    "raw_book_6_4 = loc + '/' + 'DieRauber_3.html'\n",
    "raw_book_6_5 = loc + '/' + 'DieRauber_4.html'\n",
    "raw_book_6_6 = loc + '/' + 'DieRauber_5.html'\n",
    "raw_book_6_7 = loc + '/' + 'DieRauber_6.html'\n",
    "raw_book_6_8 = loc + '/' + 'DieRauber_7.html'\n",
    "raw_book_6_9 = loc + '/' + 'DieRauber_8.html'\n",
    "raw_book_6 = [raw_book_6_1, raw_book_6_2, raw_book_6_3, raw_book_6_4,\n",
    "              raw_book_6_5, raw_book_6_6, raw_book_6_7, raw_book_6_8,\n",
    "              raw_book_6_9]\n",
    "\n",
    "\n",
    "#book7 - Die Verschwörung des fiesco zu Genua 13 - DieVersch_00.html\n",
    "raw_book_7_1 = loc + '/' + 'DieVersch_00.html'\n",
    "raw_book_7_2 = loc + '/' + 'DieVersch_01.html'\n",
    "raw_book_7_3 = loc + '/' + 'DieVersch_02.html'\n",
    "raw_book_7_4 = loc + '/' + 'DieVersch_03.html'\n",
    "raw_book_7_5 = loc + '/' + 'DieVersch_04.html'\n",
    "raw_book_7_6 = loc + '/' + 'DieVersch_05.html'\n",
    "raw_book_7_7 = loc + '/' + 'DieVersch_06.html'\n",
    "raw_book_7_8 = loc + '/' + 'DieVersch_07.html'\n",
    "raw_book_7_9 = loc + '/' + 'DieVersch_08.html'\n",
    "raw_book_7_10 = loc + '/' + 'DieVersch_09.html'\n",
    "raw_book_7_11 = loc + '/' + 'DieVersch_10.html'\n",
    "raw_book_7_12 = loc + '/' + 'DieVersch_11.html'\n",
    "raw_book_7_13 = loc + '/' + 'DieVersch_12.html'\n",
    "raw_book_7 = [raw_book_7_1, raw_book_7_2, raw_book_7_3, raw_book_7_4,\n",
    "              raw_book_7_5, raw_book_7_6, raw_book_7_7, raw_book_7_8,\n",
    "              raw_book_7_9, raw_book_7_10, raw_book_7_11, raw_book_7_12,\n",
    "              raw_book_7_13]\n",
    "\n",
    "\n",
    "#book8 - Don Carlos - 10 - DonCarlos_0.html\n",
    "raw_book_8_1 = loc + '/' + 'DonCarlos_0.html'\n",
    "raw_book_8_2 = loc + '/' + 'DonCarlos_1.html'\n",
    "raw_book_8_3 = loc + '/' + 'DonCarlos_2.html'\n",
    "raw_book_8_4 = loc + '/' + 'DonCarlos_3.html'\n",
    "raw_book_8_5 = loc + '/' + 'DonCarlos_4.html'\n",
    "raw_book_8_6 = loc + '/' + 'DonCarlos_5.html'\n",
    "raw_book_8_7 = loc + '/' + 'DonCarlos_6.html'\n",
    "raw_book_8_8 = loc + '/' + 'DonCarlos_7.html'\n",
    "raw_book_8_9 = loc + '/' + 'DonCarlos_8.html'\n",
    "raw_book_8_10 = loc + '/' + 'DonCarlos_9.html'\n",
    "raw_book_8 = [raw_book_8_1, raw_book_8_2, raw_book_8_3, raw_book_8_4,\n",
    "              raw_book_8_5, raw_book_8_6, raw_book_8_7, raw_book_8_8,\n",
    "              raw_book_8_9]\n",
    "\n",
    "\n",
    "#book9 - Kabale und Liebe 7 - KabaleUnd_0.html\n",
    "raw_book_9_1 = loc + '/' + 'KabaleUnd_0.html'\n",
    "raw_book_9_2 = loc + '/' + 'KabaleUnd_1.html'\n",
    "raw_book_9_3 = loc + '/' + 'KabaleUnd_2.html'\n",
    "raw_book_9_4 = loc + '/' + 'KabaleUnd_3.html'\n",
    "raw_book_9_5 = loc + '/' + 'KabaleUnd_4.html'\n",
    "raw_book_9_6 = loc + '/' + 'KabaleUnd_5.html'\n",
    "raw_book_9_7 = loc + '/' + 'KabaleUnd_6.html'\n",
    "raw_book_9 = [raw_book_9_1, raw_book_9_2, raw_book_9_3, raw_book_9_4,\n",
    "              raw_book_9_5, raw_book_9_6, raw_book_9_7]\n",
    "\n",
    "\n",
    "#book10- Maria Stuart - 7 - MariaStuart_0.html\n",
    "raw_book_10_1 = loc + '/' + 'MariaStuart_0.html'\n",
    "raw_book_10_2 = loc + '/' + 'MariaStuart_1.html'\n",
    "raw_book_10_3 = loc + '/' + 'MariaStuart_2.html'\n",
    "raw_book_10_4 = loc + '/' + 'MariaStuart_3.html'\n",
    "raw_book_10_5 = loc + '/' + 'MariaStuart_4.html'\n",
    "raw_book_10_6 = loc + '/' + 'MariaStuart_5.html'\n",
    "raw_book_10_7 = loc + '/' + 'MariaStuart_6.html'\n",
    "raw_book_10 = [raw_book_10_1, raw_book_10_2, raw_book_10_3, raw_book_10_4,\n",
    "               raw_book_10_5, raw_book_10_6, raw_book_10_7]\n",
    "\n",
    "\n",
    "#book11- Semele\n",
    "raw_book_11_1 = loc + '/' + 'Semele_0.html'\n",
    "raw_book_11_2 = loc + '/' + 'Semele_1.html'\n",
    "raw_book_11_3 = loc + '/' + 'Semele_2.html'\n",
    "raw_book_11 = [raw_book_11_1, raw_book_11_2, raw_book_11_3]\n",
    "\n",
    "\n",
    "#book12- Wallenstein\n",
    "raw_book_12_1 = loc + '/' + 'Wallenstein_0.html'\n",
    "raw_book_12_2 = loc + '/' + 'Wallenstein_1.html'\n",
    "raw_book_12_3 = loc + '/' + 'Wallenstein_2.html'\n",
    "raw_book_12_4 = loc + '/' + 'Wallenstein_3.html'\n",
    "raw_book_12_5 = loc + '/' + 'Wallenstein_4.html'\n",
    "raw_book_12_6 = loc + '/' + 'Wallenstein_5.html'\n",
    "raw_book_12_7 = loc + '/' + 'Wallenstein_6.html'\n",
    "raw_book_12_8 = loc + '/' + 'Wallenstein_7.html'\n",
    "raw_book_12_9 = loc + '/' + 'Wallenstein_8.html'\n",
    "raw_book_12_10 = loc + '/' + 'Wallenstein_9.html'\n",
    "raw_book_12_11 = loc + '/' + 'Wallenstein_10.html'\n",
    "raw_book_12_12 = loc + '/' + 'Wallenstein_11.html'\n",
    "raw_book_12_13 = loc + '/' + 'Wallenstein_12.html'\n",
    "raw_book_12_14 = loc + '/' + 'Wallenstein_13.html'\n",
    "raw_book_12_15 = loc + '/' + 'Wallenstein_14.html'\n",
    "raw_book_12_16 = loc + '/' + 'Wallenstein_15.html'\n",
    "raw_book_12_17 = loc + '/' + 'Wallenstein_16.html'\n",
    "raw_book_12_18 = loc + '/' + 'Wallenstein_17.html'\n",
    "raw_book_12_19 = loc + '/' + 'Wallenstein_18.html'\n",
    "raw_book_12_20 = loc + '/' + 'Wallenstein_19.html'\n",
    "raw_book_12_21 = loc + '/' + 'Wallenstein_20.html'\n",
    "raw_book_12 = [raw_book_12_1, raw_book_12_2, raw_book_12_3, raw_book_12_4,\n",
    "               raw_book_12_5, raw_book_12_6, raw_book_12_7, raw_book_12_8,\n",
    "               raw_book_12_9, raw_book_12_10, raw_book_12_11, raw_book_12_12,\n",
    "               raw_book_12_13, raw_book_12_14, raw_book_12_15, raw_book_12_16,\n",
    "               raw_book_12_17, raw_book_12_18, raw_book_12_19, raw_book_12_20,\n",
    "               raw_book_12_21\n",
    "              ]\n",
    "\n",
    "#book13- Wilhelm Tell\n",
    "raw_book_13_1 = loc + '/' + 'WilhelmTell_0.html'\n",
    "raw_book_13_2 = loc + '/' + 'WilhelmTell_1.html'\n",
    "raw_book_13_3 = loc + '/' + 'WilhelmTell_3.html'\n",
    "raw_book_13_4 = loc + '/' + 'WilhelmTell_4.html'\n",
    "raw_book_13_5 = loc + '/' + 'WilhelmTell_5.html'\n",
    "raw_book_13_6 = loc + '/' + 'WilhelmTell_6.html'\n",
    "raw_book_13_7 = loc + '/' + 'WilhelmTell_7.html'\n",
    "raw_book_13 = [raw_book_13_1, raw_book_13_2, raw_book_13_3, raw_book_13_4,\n",
    "               raw_book_13_5, raw_book_13_6, raw_book_13_7\n",
    "              ]\n",
    "\n",
    "\n",
    "raw_books_schiller = [(raw_book_1,1),\n",
    "                      (raw_book_2,2),\n",
    "                      (raw_book_3,7),\n",
    "                      (raw_book_4,1),\n",
    "                      (raw_book_5,10),\n",
    "                      (raw_book_6,9),\n",
    "                      (raw_book_7,13),\n",
    "                      (raw_book_8,9),\n",
    "                      (raw_book_9,7),\n",
    "                      (raw_book_10,7),\n",
    "                      (raw_book_11,3),\n",
    "                      (raw_book_12,21),\n",
    "                      (raw_book_13,7)\n",
    "                     ]\n",
    "\n",
    "print(type(raw_books_schiller))\n",
    "\n",
    "\n",
    "# NLTK's default German stopwords\n",
    "default_stopwords = set(nltk.corpus.stopwords.words('german'))\n",
    "custom_stopwords = set((u'–', u'dass', u'mehr'))\n",
    "\n",
    "\n",
    "all_stopwords = default_stopwords | custom_stopwords\n",
    "\n",
    "def stick_the_books_together(book, number):\n",
    "    \"\"\" This function get all the different html books and put them to one book text/unicode string.\n",
    "    \n",
    "        input: \n",
    "        - book: text string\n",
    "        the path of the given book\n",
    "        \n",
    "        - number: integer\n",
    "        how many books are published\n",
    "        \n",
    "        \n",
    "        output:\n",
    "        result: text of all the books\n",
    "        all the books    \n",
    "    \"\"\"\n",
    "    all_raw_text = ''\n",
    "    \n",
    "    for i in range(number):\n",
    "        print(i)\n",
    "        path = corpus_root + book[i]\n",
    "        html = open(path).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        raw = soup.get_text()\n",
    "        all_raw_text = all_raw_text + raw\n",
    "    \n",
    "    return(all_raw_text)\n",
    "\n",
    "wallenstein_all_raw = stick_the_books_together(raw_book_12, 21)\n",
    "#print(type(wallenstein_all_raw))\n",
    "\n",
    "\n",
    "def text_analysis(all_raw_text, start = int, end = int):\n",
    "    \"\"\" This function get all_raw_text - maybe from stick_the_books_together() - \n",
    "    and did some analysis to return some wordlists\n",
    "    \n",
    "    the start and end characters are her to sort some type out at the beginning and the end of the book\n",
    "    \n",
    "    input:\n",
    "    - all_raw_text: unicode\n",
    "    - start character: int\n",
    "    - end character: int\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    raw = all_raw_text[start:]\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    \n",
    "    text = nltk.Text(tokens)\n",
    "    \n",
    "    # Remove single-character tokens (mostly punctuation)\n",
    "    text = [word for word in text if len(word) > 1]\n",
    "    \n",
    "    # Remove numbers\n",
    "    words = [word for word in text if not word.isnumeric()]\n",
    "    \n",
    "    # lowercase all words (default_stopwords are lowercase too)\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in all_stopwords]\n",
    "    \n",
    "    # return the processed word list\n",
    "    return words\n",
    "\n",
    "test = text_analysis(wallenstein_all_raw, 400 ,70)\n",
    "#print(test)\n",
    "\n",
    "#Download the POS Tagger first\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "#<ConllCorpusReader in u'/home/user/Dokumente/Github/5pk/txt_edit'>\n",
    "root = '.'\n",
    "fileid = 'tiger.16012013.conll09'\n",
    "columntypes = ['ignore', 'words', 'ignore', 'ignore', 'pos']\n",
    "corp = nltk.corpus.ConllCorpusReader(root, fileid, columntypes, encoding='utf8')\n",
    "\n",
    "\n",
    "#load the tiger corpus to train the pos tagger later\n",
    "#corp = nltk.corpus.ConllCorpusReader('.', 'tiger_release_aug07.corrected.16012013.conll09',\n",
    "#                                     ['ignore', 'words', 'ignore', 'ignore', 'pos'],\n",
    "#                                     encoding='utf-8')\n",
    "\n",
    "print(corp)\n",
    "print(type(corp))\n",
    "print(corp.tagged_sents())\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "# explicitly make list, then LazySequence will traverse all items\n",
    "#https://stackoverflow.com/questions/39622121/nltk-perceptron-tagger-typeerror-lazysubsequence-object-does-not-support-ite\n",
    "tagged_sentences = [corp.tagged_sents()]\n",
    "i = math.floor(len(tagged_sentences)*0.2)\n",
    "testing_sentences = tagged_sentences[0:int(i)]\n",
    "training_sentences = tagged_sentences[int(i):]\n",
    "\n",
    "perceptron_tagger = nltk.tag.perceptron.PerceptronTagger(load=False)\n",
    "print(\"testtesttest\")\n",
    "print(training_sentences)\n",
    "print(len(training_sentences))\n",
    "perceptron_tagger.train(training_sentences)\n",
    "\n",
    "print(type(perceptron_tagger))\n",
    "\n",
    "\"\"\"the solution:\n",
    " https://stackoverflow.com/questions/39622121/nltk-perceptron-tagger-typeerror-lazysubsequence-object-does-not-support-ite\n",
    "\n",
    "Solution\n",
    "\n",
    "To work around the error, just explicitly create a list of the sentences from the nltk.corpus.brown package then random can shuffle the data properly.\n",
    "\n",
    "<code>\n",
    "    import nltk,math\n",
    "     explicitly make list, then LazySequence will traverse all items\n",
    "    tagged_sentences = [sentence for sentence in nltk.corpus.brown.tagged_sents(categories='news',tagset='universal')]\n",
    "    i = math.floor(len(tagged_sentences)*0.2)\n",
    "    testing_sentences = tagged_sentences[0:i]\n",
    "    training_sentences = tagged_sentences[i:]\n",
    "    perceptron_tagger = nltk.tag.perceptron.PerceptronTagger(load=False)\n",
    "    perceptron_tagger.train(training_sentences)\n",
    "\n",
    "</code>\n",
    "no error, yea!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# set a split size: use 90% for training, 10% for testing\n",
    "split_perc = 0.1\n",
    "split_size = int(len(tagged_sents) * split_perc)\n",
    "train_sents, test_sents = tagged_sents[split_size:], tagged_sents[:split_size]\n",
    "\n",
    "\n",
    "#do a pos tagging for the test text\n",
    "#test = nltk.pos_tag(test)\n",
    "#print(test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for book in raw_books_schiller:\n",
    "    path = corpus_root + book[0]\n",
    "    html = open(path).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    raw = soup.get_text()\n",
    "    print(\"########### jetzt kommt der Type\")\n",
    "    print(type(raw))\n",
    "    raw = raw[237:]\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    \n",
    "    text = nltk.Text(tokens)\n",
    "\n",
    "    # Remove single-character tokens (mostly punctuation)\n",
    "    text = [word for word in text if len(word) > 1]\n",
    "    \n",
    "    # Remove numbers\n",
    "    words = [word for word in text if not word.isnumeric()]\n",
    "    \n",
    "    # Lowercase all words (default_stopwords are lowercase too)\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in all_stopwords]\n",
    "    \n",
    "    # Calculate frequency distribution\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    \n",
    "    # Output top 20 words\n",
    "    for word, frequency in fdist.most_common(20):\n",
    "        print(u'{};{}'.format(word, frequency))\n",
    "\"\"\"\n",
    "    \n",
    "#    fdist1 = FreqDist(words)\n",
    "#    fdist1.most_common(50)\n",
    "#    \n",
    "#    vocab = sorted(set(words))\n",
    "#    \n",
    "#    for word in vocab:\n",
    "#        print(word)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "i = 0\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    print(filename)\n",
    "    f = open(path + '/' + filename)\n",
    "    soup = BeautifulSoup(f.read(), \"html5lib\")\n",
    "    i += 1\n",
    "    for i in soup.findAll(u\"\\u2018\"):\n",
    "        if i.text == u\"\\2018\" or u\"\\2019\" in i.text:\n",
    "            i.string = \"'\"\n",
    "    for i in soup.findAll(u\"\\u2019\"):\n",
    "        if i.text == u\"\\2018\" or u\"\\2019\" in i.text:\n",
    "            i.string = \"'\"\n",
    "    g = soup.get_text().encode('utf-8')\n",
    "    print(g)\n",
    "    \n",
    "    \n",
    "#for file in glob.glob('*.html'):\n",
    " #   detect()\n",
    " \n",
    " \"\"\"\n",
    "\n",
    "\"\"\"for book in raw_books_schiller:\n",
    "    print(book)\n",
    "    pcr = PlaintextCorpusReader(corpus_root + book[0],'.*')\n",
    "    pcr.fileids()\n",
    "    print(wordlist)\n",
    "    #num_chars = len(wordlist)\n",
    "    test = gutenberg.raw(wordlist)\n",
    "    num_words = gutenberg.words(wordlist)\n",
    "    num_sents = gutenberg.sents(wordlist)\n",
    "\n",
    "\n",
    "for book in raw_books_schiller:\n",
    "    num_chars = len(gutenberg.raw(book))\n",
    "    num_words = len(gutenberg.words(book))\n",
    "    num_sents = len(gutenberg.sents(book))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# database storeagefile\n",
    "db = './db/schiller.db'\n",
    "\n",
    "# connect the file/db\n",
    "conn = sqlite3.connect(db)\n",
    "c = conn.cursor()\n",
    "\n",
    "#Create the table\n",
    "c.execute('''CREATE TABLE books \n",
    "            (author int, name text, book text)''')\n",
    "\n",
    "#insert a row of data\n",
    "c.execute(\"INSERT INTO books VALUES (1, )\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "BEGIN TRANSACTION;\n",
    "CREATE TABLE IF NOT EXISTS `tbl_Buecher` (\n",
    "\t`ID`\tINTEGER NOT NULL UNIQUE,\n",
    "\t`Name`\tTEXT NOT NULL UNIQUE,\n",
    "\t`Author`\tINTEGER NOT NULL UNIQUE,\n",
    "\t`Erscheinungsjahr`\tINTEGER NOT NULL UNIQUE,\n",
    "\tPRIMARY KEY(`ID`)\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS `tbl_Autor` (\n",
    "\t`ID`\tINTEGER NOT NULL UNIQUE,\n",
    "\t`Name`\tTEXT NOT NULL UNIQUE,\n",
    "\tPRIMARY KEY(`ID`)\n",
    ");\n",
    "COMMIT;\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
