{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import zipfile\n",
    "import os.path\n",
    "import glob\n",
    "from chardet.universaldetector import UniversalDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unzip(source_filename, dest_dir):\n",
    "    \"\"\"this function unzips all the files from the epub file\n",
    "        the following functions should only need the textfiles\"\"\"\n",
    "    with zipfile.ZipFile(source_filename) as zf:\n",
    "        for member in zf.infolist():\n",
    "            # Path traversal defense copied from\n",
    "            # http://hg.python.org/cpython/file/tip/Lib/http/server.py#l789\n",
    "            words = member.filename.split('/')\n",
    "            path = dest_dir\n",
    "            for word in words[:-1]:\n",
    "                while True:\n",
    "                    drive, word = os.path.splitdrive(word)\n",
    "                    head, word = os.path.split(word)\n",
    "                    if not drive:\n",
    "                        print(\"no drive\")\n",
    "                        break\n",
    "                if word in (os.curdir, os.pardir, ''):\n",
    "                    continue\n",
    "                path = os.path.join(path, word)\n",
    "            zf.extract(member, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_txt(dir):\n",
    "    for path in glob.glob(dir + '/*.html'):\n",
    "        with open(path) as markup:\n",
    "            soup = BeautifulSoup(markup.read(),\"html5lib\" )\n",
    "        with open(\"strip_\" + path, \"w\") as f:\n",
    "            f.write(soup.get_text().encode('ISO-8859-2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect(file):\n",
    "    detector = UniversalDetector()\n",
    "    for filename in glob.glob(file):\n",
    "        print(filename.ljust(60),detector.reset())\n",
    "        for line in file(filename, 'rb'):\n",
    "            detector.feed(line)\n",
    "            if detector.done:\n",
    "                break\n",
    "    \n",
    "    detector.close()\n",
    "    print(detector.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'schiller.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e7690d334923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"schiller.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"html5lib\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ISO-8859-2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'schiller.txt'"
     ]
    }
   ],
   "source": [
    "def convert(file, coding):\n",
    "    f = open(file)\n",
    "    \n",
    "    \n",
    "    \n",
    "f = open(\"schiller.txt\")\n",
    "soup = BeautifulSoup(f.read(),\"html5lib\" )\n",
    "g = soup.get_text().encode('ISO-8859-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# coding: latin1\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#!pip3 install -U spacy\n",
    "#!python -m spacy download de\n",
    "#import spacy\n",
    "import codecs\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlite3 import Error\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from urllib.request import urlopen\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "corpus_root = '/home/user/Dokumente/Github/5pk/txt_edit'\n",
    "\n",
    "\n",
    "# The books are splitted into several files. First i have to group the \n",
    "# files for every book and put them into one textstring\n",
    "\n",
    "# html_folder\n",
    "loc = \"/html\"\n",
    "\n",
    "##############################################\n",
    "#\n",
    "# 1. Teil vom input - bereits eingelesen\n",
    "#\n",
    "##############################################\n",
    "\n",
    "\n",
    "\n",
    "#book1 - Demetrius\n",
    "raw_book_1_1 = loc + '/' + 'Demetrius.html'\n",
    "raw_book_1 = [raw_book_1_1]\n",
    "\n",
    "#book2 - Der versöhnte Menschenfeind\n",
    "raw_book_2_1 = loc + '/' + 'DerVers_0.html'\n",
    "raw_book_2_2 = loc + '/' + 'DerVers_1.html'\n",
    "raw_book_2 = [raw_book_2_1, raw_book_2_2]\n",
    "\n",
    "#book3 - Die Braut von Messina\n",
    "raw_book_3_1 = loc + '/' + 'DieBraut_0.html'\n",
    "raw_book_3_2 = loc + '/' + 'DieBraut_1.html'\n",
    "raw_book_3_3 = loc + '/' + 'DieBraut_2.html'\n",
    "raw_book_3_4 = loc + '/' + 'DieBraut_3.html'\n",
    "raw_book_3_5 = loc + '/' + 'DieBraut_4.html'\n",
    "raw_book_3_6 = loc + '/' + 'DieBraut_5.html'\n",
    "raw_book_3_7 = loc + '/' + 'DieBraut_6.html'\n",
    "raw_book_3 = [raw_book_3_1, raw_book_3_2, raw_book_3_3, raw_book_3_4, \n",
    "              raw_book_3_5, raw_book_3_6, raw_book_3_7\n",
    "             ]\n",
    "\n",
    "#book4 - Die Huldigung der Künste - 1 - DieHuldi.html\n",
    "raw_book_4_1 = loc + '/' + 'DieHuldi.html'\n",
    "raw_book_4 = [raw_book_4_1]\n",
    "\n",
    "#book5 - Die Jungfrau von Orleans - 10 - DieJungfrau_00.html\n",
    "raw_book_5_1 = loc + '/' + 'DieJungfrau_00.html'\n",
    "raw_book_5_2 = loc + '/' + 'DieJungfrau_01.html'\n",
    "raw_book_5_3 = loc + '/' + 'DieJungfrau_02.html'\n",
    "raw_book_5_4 = loc + '/' + 'DieJungfrau_03.html'\n",
    "raw_book_5_5 = loc + '/' + 'DieJungfrau_04.html'\n",
    "raw_book_5_6 = loc + '/' + 'DieJungfrau_05.html'\n",
    "raw_book_5_7 = loc + '/' + 'DieJungfrau_06.html'\n",
    "raw_book_5_8 = loc + '/' + 'DieJungfrau_07.html'\n",
    "raw_book_5_9 = loc + '/' + 'DieJungfrau_08.html'\n",
    "raw_book_5_10 = loc + '/' + 'DieJungfrau_09.html'\n",
    "raw_book_5 = [raw_book_5_1, raw_book_5_2, raw_book_5_3, raw_book_5_4,\n",
    "              raw_book_5_5, raw_book_5_6, raw_book_5_7, raw_book_5_8,\n",
    "              raw_book_5_9, raw_book_5_10]\n",
    "\n",
    "#book6 - Die Räuber - 9 - DieRauber_0.html\n",
    "raw_book_6_1 = loc + '/' + 'DieRauber_0.html'\n",
    "raw_book_6_2 = loc + '/' + 'DieRauber_1.html'\n",
    "raw_book_6_3 = loc + '/' + 'DieRauber_2.html'\n",
    "raw_book_6_4 = loc + '/' + 'DieRauber_3.html'\n",
    "raw_book_6_5 = loc + '/' + 'DieRauber_4.html'\n",
    "raw_book_6_6 = loc + '/' + 'DieRauber_5.html'\n",
    "raw_book_6_7 = loc + '/' + 'DieRauber_6.html'\n",
    "raw_book_6_8 = loc + '/' + 'DieRauber_7.html'\n",
    "raw_book_6_9 = loc + '/' + 'DieRauber_8.html'\n",
    "raw_book_6 = [raw_book_6_1, raw_book_6_2, raw_book_6_3, raw_book_6_4,\n",
    "              raw_book_6_5, raw_book_6_6, raw_book_6_7, raw_book_6_8,\n",
    "              raw_book_6_9]\n",
    "\n",
    "#book7 - Die Verschwörung des fiesco zu Genua 13 - DieVersch_00.html\n",
    "raw_book_7_1 = loc + '/' + 'DieVersch_00.html'\n",
    "raw_book_7_2 = loc + '/' + 'DieVersch_01.html'\n",
    "raw_book_7_3 = loc + '/' + 'DieVersch_02.html'\n",
    "raw_book_7_4 = loc + '/' + 'DieVersch_03.html'\n",
    "raw_book_7_5 = loc + '/' + 'DieVersch_04.html'\n",
    "raw_book_7_6 = loc + '/' + 'DieVersch_05.html'\n",
    "raw_book_7_7 = loc + '/' + 'DieVersch_06.html'\n",
    "raw_book_7_8 = loc + '/' + 'DieVersch_07.html'\n",
    "raw_book_7_9 = loc + '/' + 'DieVersch_08.html'\n",
    "raw_book_7_10 = loc + '/' + 'DieVersch_09.html'\n",
    "raw_book_7_11 = loc + '/' + 'DieVersch_10.html'\n",
    "raw_book_7_12 = loc + '/' + 'DieVersch_11.html'\n",
    "raw_book_7_13 = loc + '/' + 'DieVersch_12.html'\n",
    "raw_book_7 = [raw_book_7_1, raw_book_7_2, raw_book_7_3, raw_book_7_4,\n",
    "              raw_book_7_5, raw_book_7_6, raw_book_7_7, raw_book_7_8,\n",
    "              raw_book_7_9, raw_book_7_10, raw_book_7_11, raw_book_7_12,\n",
    "              raw_book_7_13]\n",
    "\n",
    "#book8 - Don Carlos - 10 - DonCarlos_0.html\n",
    "raw_book_8_1 = loc + '/' + 'DonCarlos_0.html'\n",
    "raw_book_8_2 = loc + '/' + 'DonCarlos_1.html'\n",
    "raw_book_8_3 = loc + '/' + 'DonCarlos_2.html'\n",
    "raw_book_8_4 = loc + '/' + 'DonCarlos_3.html'\n",
    "raw_book_8_5 = loc + '/' + 'DonCarlos_4.html'\n",
    "raw_book_8_6 = loc + '/' + 'DonCarlos_5.html'\n",
    "raw_book_8_7 = loc + '/' + 'DonCarlos_6.html'\n",
    "raw_book_8_8 = loc + '/' + 'DonCarlos_7.html'\n",
    "raw_book_8_9 = loc + '/' + 'DonCarlos_8.html'\n",
    "raw_book_8_10 = loc + '/' + 'DonCarlos_9.html'\n",
    "raw_book_8 = [raw_book_8_1, raw_book_8_2, raw_book_8_3, raw_book_8_4,\n",
    "              raw_book_8_5, raw_book_8_6, raw_book_8_7, raw_book_8_8,\n",
    "              raw_book_8_9]\n",
    "\n",
    "#book9 - Kabale und Liebe 7 - KabaleUnd_0.html\n",
    "raw_book_9_1 = loc + '/' + 'KabaleUnd_0.html'\n",
    "raw_book_9_2 = loc + '/' + 'KabaleUnd_1.html'\n",
    "raw_book_9_3 = loc + '/' + 'KabaleUnd_2.html'\n",
    "raw_book_9_4 = loc + '/' + 'KabaleUnd_3.html'\n",
    "raw_book_9_5 = loc + '/' + 'KabaleUnd_4.html'\n",
    "raw_book_9_6 = loc + '/' + 'KabaleUnd_5.html'\n",
    "raw_book_9_7 = loc + '/' + 'KabaleUnd_6.html'\n",
    "raw_book_9 = [raw_book_9_1, raw_book_9_2, raw_book_9_3, raw_book_9_4,\n",
    "              raw_book_9_5, raw_book_9_6, raw_book_9_7]\n",
    "\n",
    "#book10- Maria Stuart - 7 - MariaStuart_0.html\n",
    "raw_book_10_1 = loc + '/' + 'MariaStuart_0.html'\n",
    "raw_book_10_2 = loc + '/' + 'MariaStuart_1.html'\n",
    "raw_book_10_3 = loc + '/' + 'MariaStuart_2.html'\n",
    "raw_book_10_4 = loc + '/' + 'MariaStuart_3.html'\n",
    "raw_book_10_5 = loc + '/' + 'MariaStuart_4.html'\n",
    "raw_book_10_6 = loc + '/' + 'MariaStuart_5.html'\n",
    "raw_book_10_7 = loc + '/' + 'MariaStuart_6.html'\n",
    "raw_book_10 = [raw_book_10_1, raw_book_10_2, raw_book_10_3, raw_book_10_4,\n",
    "               raw_book_10_5, raw_book_10_6, raw_book_10_7]\n",
    "\n",
    "#book11- Semele\n",
    "raw_book_11_1 = loc + '/' + 'Semele_0.html'\n",
    "raw_book_11_2 = loc + '/' + 'Semele_1.html'\n",
    "raw_book_11_3 = loc + '/' + 'Semele_2.html'\n",
    "raw_book_11 = [raw_book_11_1, raw_book_11_2, raw_book_11_3]\n",
    "\n",
    "#book12- Wallenstein\n",
    "raw_book_12_1 = loc + '/' + 'Wallenstein_0.html'\n",
    "raw_book_12_2 = loc + '/' + 'Wallenstein_1.html'\n",
    "raw_book_12_3 = loc + '/' + 'Wallenstein_2.html'\n",
    "raw_book_12_4 = loc + '/' + 'Wallenstein_3.html'\n",
    "raw_book_12_5 = loc + '/' + 'Wallenstein_4.html'\n",
    "raw_book_12_6 = loc + '/' + 'Wallenstein_5.html'\n",
    "raw_book_12_7 = loc + '/' + 'Wallenstein_6.html'\n",
    "raw_book_12_8 = loc + '/' + 'Wallenstein_7.html'\n",
    "raw_book_12_9 = loc + '/' + 'Wallenstein_8.html'\n",
    "raw_book_12_10 = loc + '/' + 'Wallenstein_9.html'\n",
    "raw_book_12_11 = loc + '/' + 'Wallenstein_10.html'\n",
    "raw_book_12_12 = loc + '/' + 'Wallenstein_11.html'\n",
    "raw_book_12_13 = loc + '/' + 'Wallenstein_12.html'\n",
    "raw_book_12_14 = loc + '/' + 'Wallenstein_13.html'\n",
    "raw_book_12_15 = loc + '/' + 'Wallenstein_14.html'\n",
    "raw_book_12_16 = loc + '/' + 'Wallenstein_15.html'\n",
    "raw_book_12_17 = loc + '/' + 'Wallenstein_16.html'\n",
    "raw_book_12_18 = loc + '/' + 'Wallenstein_17.html'\n",
    "raw_book_12_19 = loc + '/' + 'Wallenstein_18.html'\n",
    "raw_book_12_20 = loc + '/' + 'Wallenstein_19.html'\n",
    "raw_book_12_21 = loc + '/' + 'Wallenstein_20.html'\n",
    "raw_book_12 = [raw_book_12_1, raw_book_12_2, raw_book_12_3, raw_book_12_4,\n",
    "               raw_book_12_5, raw_book_12_6, raw_book_12_7, raw_book_12_8,\n",
    "               raw_book_12_9, raw_book_12_10, raw_book_12_11, raw_book_12_12,\n",
    "               raw_book_12_13, raw_book_12_14, raw_book_12_15, raw_book_12_16,\n",
    "               raw_book_12_17, raw_book_12_18, raw_book_12_19, raw_book_12_20,\n",
    "               raw_book_12_21\n",
    "              ]\n",
    "\n",
    "#book13- Wilhelm Tell\n",
    "raw_book_13_1 = loc + '/' + 'WilhelmTell_0.html'\n",
    "raw_book_13_2 = loc + '/' + 'WilhelmTell_1.html'\n",
    "raw_book_13_3 = loc + '/' + 'WilhelmTell_3.html'\n",
    "raw_book_13_4 = loc + '/' + 'WilhelmTell_4.html'\n",
    "raw_book_13_5 = loc + '/' + 'WilhelmTell_5.html'\n",
    "raw_book_13_6 = loc + '/' + 'WilhelmTell_6.html'\n",
    "raw_book_13_7 = loc + '/' + 'WilhelmTell_7.html'\n",
    "raw_book_13 = [raw_book_13_1, raw_book_13_2, raw_book_13_3, raw_book_13_4,\n",
    "               raw_book_13_5, raw_book_13_6, raw_book_13_7\n",
    "              ]\n",
    "\n",
    "##############################################\n",
    "#\n",
    "# 1. Teil des inputs ist beendet\n",
    "#\n",
    "##############################################\n",
    "\n",
    "\n",
    "raw_books_schiller = [(raw_book_1,1),\n",
    "                      (raw_book_2,2),\n",
    "                      (raw_book_3,7),\n",
    "                      (raw_book_4,1),\n",
    "                      (raw_book_5,10),\n",
    "                      (raw_book_6,9),\n",
    "                      (raw_book_7,13),\n",
    "                      (raw_book_8,9),\n",
    "                      (raw_book_9,7),\n",
    "                      (raw_book_10,7),\n",
    "                      (raw_book_11,3),\n",
    "                      (raw_book_12,21),\n",
    "                      (raw_book_13,7)\n",
    "                     ]\n",
    "\n",
    "print(type(raw_books_schiller))\n",
    "\n",
    "\n",
    "# NLTK's default German stopwords\n",
    "default_stopwords = set(nltk.corpus.stopwords.words('german'))\n",
    "custom_stopwords = set((u'–', u'dass', u'mehr', u'0000ff', u'000ff', u'0em', u'1.2', u'1em', u'2em', u'a.sgc-2',\n",
    "                        u'center', u'color', u'div.sgc-1', u'div.sgc-5', u'div.sgc-toc-level-1',\n",
    "                        u'div.sgc-toc-level-2', u'div.sgc-toc-level-3', u'div.sgc-toc-level-4',\n",
    "                        u'div.sgc-toc-level-5', u'div.sgc-toc-level-6', u'div.sgc-toc-title', u'document',\n",
    "                        u'e-artnow', u'font-face', u'font-size', u'h1.sgc-4', u'h2.sgc-1', u'h2.sgc-3', \n",
    "                        u'inhaltsverzeichnis', u'line-height', u'margin-bottom', u'margin-left', u'margin:20',\n",
    "                        u' ', u'text-align', u'text-decoration'\n",
    "                       ))\n",
    "\n",
    "all_stopwords = default_stopwords | custom_stopwords\n",
    "\n",
    "def stick_the_books_together(book, number):\n",
    "    \"\"\" This function get all the different html books and put them to one book text/unicode string.\n",
    "    \n",
    "        input: \n",
    "        - book: text string\n",
    "        the path of the given book\n",
    "        \n",
    "        - number: integer\n",
    "        how many books are published\n",
    "        \n",
    "        \n",
    "        output:\n",
    "        result: text of all the books\n",
    "        all the books    \n",
    "    \"\"\"\n",
    "    all_raw_text = ''\n",
    "    \n",
    "    for i in range(number):\n",
    "        #print(i)\n",
    "        path = corpus_root + book[i]\n",
    "        html = open(path).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        raw = soup.get_text()\n",
    "        all_raw_text = all_raw_text + raw\n",
    "    \n",
    "    return(all_raw_text)\n",
    "\n",
    "\n",
    "def create_connection(database):\n",
    "    \"\"\" create a database connection to the SQLite database \n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(database)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None \n",
    "\n",
    "\n",
    "def create_table(conn, create_table_sql):\n",
    "    \"\"\" create a table from the create_table_sql statement\n",
    "        param conn: Connection object\n",
    "        param create_table_sql: a CREATE TABLE statement\n",
    "        return:\"\"\"\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(create_table_sql)\n",
    "    except Error as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "def create_the_book_db():\n",
    "    \n",
    "    database = './db/5pk.db'\n",
    "    \n",
    "    sql_create_tbl_Authors = \"\"\" CREATE TABLE IF NOT EXISTS tbl_Authors (\n",
    "                                    Author_ID integer NOT NULL Primary Key,\n",
    "                                    Author_NAME text NOT NULL);\"\"\"\n",
    "    \n",
    "    sql_create_tbl_Books = \"\"\" CREATE TABLE IF NOT EXISTS tbl_Books (\n",
    "                                    Book_ID integer NOT NULL Primary Key,\n",
    "                                    Book_NAME text NOT NULL,\n",
    "                                    Author_ID integer NOT NULL,\n",
    "                                    Year integer NOT NULL,\n",
    "                                    FOREIGN KEY (Author_ID) REFERENCES\n",
    "                                    tbl_Authors(Author_ID));\"\"\"\n",
    "    \n",
    "    sql_create_tbl_Sentences = \"\"\" CREATE TABLE IF NOT EXISTS tbl_Sentences (\n",
    "                                    Sentence_ID integer Primary Key,\n",
    "                                    Book_ID integer NOT NULL,\n",
    "                                    Sentence TEXT NOT NULL,\n",
    "                                    FOREIGN KEY (Book_ID) REFERENCES\n",
    "                                    tbl_Books(Book_ID));\"\"\"\n",
    "\n",
    "    \n",
    "    sql_create_ind_Authors_Author_ID = \"\"\" CREATE INDEX ind_Authors_Author_ID\n",
    "                                            ON tbl_Authors(Author_ID);\"\"\"\n",
    "    \n",
    "    sql_create_ind_Books_Book_ID = \"\"\" CREATE INDEX ind_Books_Book_ID\n",
    "                                        ON tbl_Books(Book_ID);\"\"\"\n",
    "    \n",
    "    sql_create_ind_Sentences_Sentence_ID = \"\"\" CREATE INDEX ind_Sentences_Sentence_ID \n",
    "                                                ON tbl_Sentences(Sentence_ID);\"\"\"\n",
    "    \n",
    "    # Connecting to the database file\n",
    "    conn = create_connection('./db/5pk.db')\n",
    "\n",
    "    if conn is not None:\n",
    "        #create Authors table\n",
    "        create_table(conn, sql_create_tbl_Authors)\n",
    "        #create Books table\n",
    "        create_table(conn, sql_create_tbl_Books)\n",
    "        #create Sentences\n",
    "        create_table(conn, sql_create_tbl_Sentences)\n",
    "        \n",
    "        #create the Indexes\n",
    "        create_table(conn, sql_create_ind_Authors_Author_ID)\n",
    "        create_table(conn, sql_create_ind_Books_Book_ID)\n",
    "        create_table(conn, sql_create_ind_Sentences_Sentence_ID)\n",
    "    else:\n",
    "        print(\"ERROR! cannot create the database connection.\")\n",
    "\n",
    "\n",
    "def create_author(conn, author):\n",
    "    \"\"\"\n",
    "    Create a new Author\n",
    "    :param conn:\n",
    "    :param author:\n",
    "    :return: Author_ID\n",
    "    \"\"\"\n",
    "    sql = '''INSERT INTO tbl_Authors(Author_NAME, )'''\n",
    "        \n",
    "def create_book(conn, book):\n",
    "    \"\"\"\n",
    "    Create a new book into the tbl_Books\n",
    "    :param conn:\n",
    "    :param book:\n",
    "    :return: book id\n",
    "    \"\"\"\n",
    "    sql = '''INSERT INTO tbl_Books(Book_NAME, Author_ID, Year)\n",
    "            VALUES(?,?,?) '''\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, book)\n",
    "    \n",
    "    return cur.lastrowid\n",
    "\n",
    "\n",
    "def create_sentences(conn, sentence):\n",
    "    \"\"\"\n",
    "    Create a new Sentence\n",
    "    :param conn:\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sql = ''' INSERT INTO tbl_Sentences(Book_ID, Sentence)\n",
    "            Values(?,?) '''\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, sentence)\n",
    "    return cur.lastrowid\n",
    "        \n",
    "\n",
    "def select_all_sentences(conn):\n",
    "    \"\"\"\n",
    "    Query all rows in the tbl_Sentences table\n",
    "    :param conn: the connection object\n",
    "    :return: print every row\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT * FROM tbl_Sentences\")\n",
    "    \n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    for row in rows:\n",
    "        print(row)\n",
    "        \n",
    "        \n",
    "def select_sentences_by_sentence_id(conn, Book_ID):\n",
    "    \"\"\"\n",
    "    Query sentence by ID\n",
    "    :param conn: the connection object\n",
    "    :param Book_ID: \n",
    "    :return: print every row\n",
    "    \"\"\"\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELCET * FROM tbl_Sentences WHERE Book_ID=?\", (Book_ID,))\n",
    "    \n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    for row in rows:\n",
    "        print(row)\n",
    "    \n",
    "def fill_the_db_with_books(list_of_books):\n",
    "    db = './db/5pk.db'    \n",
    "    conn = create_connection(db)\n",
    "    conn.text_factory = str\n",
    "    #INSERT INTO tbl_Books(Book_NAME, Author_ID, Year)\n",
    "    df = pd.DataFrame(list_of_books, columns=['Book_NAME', 'Author_ID', 'Year'])\n",
    "    #put the book list into the db\n",
    "    df.to_sql(\"tbl_Books\", conn, if_exists=\"append\")\n",
    "    \n",
    "    #check if everything is really in the db table\n",
    "    test = pd.read_sql_query(\"select * from tbl_Books;\", conn)\n",
    "    return test\n",
    "\n",
    "def fill_the_db_with_sentence(df_of_sentences):\n",
    "    db = './db/5pk.db'\n",
    "    conn = create_connection(db)\n",
    "    conn.text_factory = str\n",
    "    #INSERT INTO tbl_Sentence(Sentence, Book_ID)\n",
    "    #df = pd.DataFrame(list_of_sentences, columns=['Sentence', 'Book_ID'])\n",
    "    df = df_of_sentences\n",
    "    #put the sentences from a book into the db\n",
    "    df.to_sql(\"tbl_Sentences\", conn, if_exists=\"append\")\n",
    "    \n",
    "    #check if everything is really in the db table\n",
    "    test = pd.read_sql_query(\"select * from tbl_Sentences;\", conn)\n",
    "    return test\n",
    "\n",
    "def fill_the_db_with_authors(list_of_authors):\n",
    "    db = './db/5pk.db'\n",
    "    conn = create_connection(db)\n",
    "    conn.text_factory = str\n",
    "    #INSERT INTO tbl_Authors(Author_NAME)\n",
    "    df = pd.DataFrame(list_of_authors, columns=['Author_NAME'])\n",
    "    #put the author list into the db Authors\n",
    "    df.to_sql(\"tbl_Authors\", conn, if_exists=\"append\")\n",
    "    \n",
    "    #check if everything is really in the db author\n",
    "    test = pd.read_sql_query(\"select * from tbl_Authors;\", conn)\n",
    "    return test\n",
    "\n",
    "# book_list for the tbl_Books\n",
    "# this is the first lis in december 2017\n",
    "book_list = [('Demetrius', 0, 1805),\n",
    "               ('Der versöhnte Menschenfeind', 0, 1790),\n",
    "               ('Die Braut von Messina', 0, 1803),\n",
    "               ('Die Huldigung der Künste', 0, 1804),\n",
    "               ('Die Jungfrau von Orleans', 0, 1801),\n",
    "               ('Die Räuber', 0, 1781),\n",
    "               ('Die Verschwörung des Fiesco zu Genua', 0, 1784),\n",
    "               ('Don Carlos', 0, 1787),\n",
    "               ('Kabale und Liebe', 0, 1783),\n",
    "               ('Maria Stuart', 0, 1800),\n",
    "               ('Semele', 0, 1782),\n",
    "               ('Wallenstein', 0, 1799),\n",
    "               ('Wilhelm Tell', 0, 1804)              \n",
    "              ]\n",
    "\n",
    "author_list = [('Friedrich Schiller')]\n",
    "\n",
    "\n",
    "\n",
    "#create the book DB in the beginning, only once\n",
    "#create_the_book_db()\n",
    "#fill_the_db_with_authors(author_list)\n",
    "#fill_the_db_with_books(book_list)\n",
    "\n",
    "\n",
    "def process_every_text_and_put_the_sentences_into_a_df(list_of_books):\n",
    "    \"\"\"\n",
    "    :param list_of_books: [(\"name of book\", number of books)]: \n",
    "    [(raw_book_1,1),(raw_book_2,2)]\n",
    "    \n",
    "    :param conn: connection to the Database\n",
    "    \n",
    "    1. the list of book has to processed with the stick_the_books_together function\n",
    "    2. this raw text get the number i from the book list and this is the Book_ID number for the tbl_Books\n",
    "    \"\"\"\n",
    "    \n",
    "    Book_ID = list_of_books[0][0]\n",
    "    print(Book_ID)\n",
    "\n",
    "#process_every_text_and_put_the_sentences_into_a_df(raw_books_schiller)\n",
    "\n",
    "def create_sentences_list(text):\n",
    "    s =  sent_tokenize(text)\n",
    "    #remove the \\n stuff\n",
    "    \n",
    "    return s\n",
    "\n",
    "#sentences = create_sentences_list(wallenstein_all_raw)\n",
    "\n",
    "\n",
    "#for i in sentences:\n",
    "#    print(i)\n",
    "\n",
    "#test = nltk.Text(sentences)\n",
    "#print(len(test))\n",
    "\n",
    "\"\"\"\n",
    "following code is taken from\n",
    "https://github.com/AlliedToasters/nlp/blob/master/4.4.2_Challenge_0.ipynb\n",
    "\"\"\"\n",
    "\n",
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\"\"\"    \n",
    "# Load and clean the data.\n",
    "wallenstein_all_raw = stick_the_books_together(raw_book_12, 21)\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "wallenstein = re.sub(r'Chapter \\d+', '', wallenstein_all_raw)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "wallenstein = text_cleaner(wallenstein)\n",
    "\n",
    "\n",
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "#persuasion_doc = nlp(persuasion)\n",
    "\n",
    "\n",
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "#ersuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents)# + persuasion_sents)\n",
    "sentences.head()\n",
    "\"\"\"\n",
    "\n",
    "def text_analysis(all_raw_text, start = int, end = int):\n",
    "    \"\"\" This function get all_raw_text - maybe from stick_the_books_together() - \n",
    "    and did some analysis to return some wordlists\n",
    "    \n",
    "    the start and end characters are her to sort some type out at the beginning and the end of the book\n",
    "    \n",
    "    input:\n",
    "    - all_raw_text: unicode\n",
    "    - start character: int\n",
    "    - end character: int    \n",
    "    \"\"\"\n",
    "    \n",
    "    raw = all_raw_text[start:]\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    \n",
    "    text = nltk.Text(tokens)\n",
    "    \n",
    "    # Remove single-character tokens (mostly punctuation)\n",
    "    text = [word for word in text if len(word) > 1]\n",
    "    \n",
    "    # Remove numbers\n",
    "    words = [word for word in text if not word.isnumeric()]\n",
    "    \n",
    "    # lowercase all words (default_stopwords are lowercase too)\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in all_stopwords]\n",
    "    \n",
    "    # return the processed word list\n",
    "    return words\n",
    "\n",
    "#test = text_analysis(wallenstein_all_raw, 400 ,70)\n",
    "#test = sorted(set(test))\n",
    "#for i in test:\n",
    "#    print(i)\n",
    "\n",
    "\n",
    "#this could be a function to \n",
    "# first - stick the books together\n",
    "# second - create sentences\n",
    "# third - made a DataFrame\n",
    "# fourth - append another column (Book_ID)\n",
    "# result is a Sentence-DF ready for sql input\n",
    "\n",
    "def stick_the_books_create_sentences_create_df(Book_ID, book_files, number_of_books):\n",
    "    text = stick_the_books_together(book_files, number_of_books)\n",
    "    text = create_sentences_list(text)\n",
    "    \n",
    "    text = pd.DataFrame(text, columns = [\"Sentence\"])\n",
    "    text['Book_ID'] = Book_ID\n",
    "    return text\n",
    "\n",
    "\n",
    "def put_all_book_sentences_into_the_db():\n",
    "    Demetrius = stick_the_books_create_sentences_create_df(0, raw_book_1, 1)\n",
    "    DerversoehnteMenschenfeind = stick_the_books_create_sentences_create_df(1, raw_book_2, 2)\n",
    "    DieBrautvonMessina = stick_the_books_create_sentences_create_df(2, raw_book_3, 7)\n",
    "    DieHuldigungderKuenste = stick_the_books_create_sentences_create_df(3, raw_book_4, 1)\n",
    "    DieJungfrauvonOrleans = stick_the_books_create_sentences_create_df(4, raw_book_5, 10)\n",
    "    DieRaeuber = stick_the_books_create_sentences_create_df(5, raw_book_6, 9)\n",
    "    DieVerschwoerungdesFiescozuGenua = stick_the_books_create_sentences_create_df(6, raw_book_7, 13)\n",
    "    DonCarlos = stick_the_books_create_sentences_create_df(7, raw_book_8, 9)\n",
    "    KabaleundLiebe = stick_the_books_create_sentences_create_df(8, raw_book_9, 7)\n",
    "    MariaStuart = stick_the_books_create_sentences_create_df(9, raw_book_10, 7)\n",
    "    Semele = stick_the_books_create_sentences_create_df(10, raw_book_11, 3)\n",
    "    Wallenstein = stick_the_books_create_sentences_create_df(11, raw_book_12, 21)\n",
    "    WilhelmTell = stick_the_books_create_sentences_create_df(12, raw_book_13, 7)\n",
    "    \n",
    "    ## Fill the Dataframes into the DB\n",
    "    sentence_db = fill_the_db_with_sentence(Demetrius)\n",
    "    sentence_db = fill_the_db_with_sentence(DerversoehnteMenschenfeind)\n",
    "    sentence_db = fill_the_db_with_sentence(DieBrautvonMessina)\n",
    "    sentence_db = fill_the_db_with_sentence(DieHuldigungderKuenste)\n",
    "    sentence_db = fill_the_db_with_sentence(DieJungfrauvonOrleans)\n",
    "    sentence_db = fill_the_db_with_sentence(DieRaeuber)\n",
    "    sentence_db = fill_the_db_with_sentence(DieVerschwoerungdesFiescozuGenua)\n",
    "    sentence_db = fill_the_db_with_sentence(DonCarlos)\n",
    "    sentence_db = fill_the_db_with_sentence(KabaleundLiebe)\n",
    "    sentence_db = fill_the_db_with_sentence(MariaStuart)\n",
    "    sentence_db = fill_the_db_with_sentence(Semele)\n",
    "    sentence_db = fill_the_db_with_sentence(Wallenstein)\n",
    "    sentence_db = fill_the_db_with_sentence(WilhelmTell)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(sentence_db)\n",
    "\n",
    "#put_all_book_sentences_into_the_db()\n",
    "\n",
    "def read_the_sentences_table():\n",
    "    db = './db/5pk.db'\n",
    "    conn = create_connection(db)\n",
    "    conn.text_factory = str\n",
    "    \n",
    "    #check if everything is really in the db table\n",
    "    return pd.read_sql_query(\"select * from tbl_Sentences;\", conn)\n",
    "\n",
    "#read_the_sentences_table()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports Done\n",
      "cleaned_sentences df into string - Done\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' objects are mutable, thus they cannot be hashed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mstat_output\u001b[0;34m(text)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/repoze/lru/__init__.py\u001b[0m in \u001b[0;36mcached_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/repoze/lru/__init__.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookups\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhits\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         raise TypeError('{0!r} objects are mutable, thus they cannot be'\n\u001b[0;32m-> 1045\u001b[0;31m                         ' hashed'.format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' objects are mutable, thus they cannot be hashed"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#!pip3 install -U nltk\n",
    "import spacy\n",
    "import pickle\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sqlite3 import Error\n",
    "from collections import Counter\n",
    "\n",
    "print(\"imports Done\")\n",
    "\n",
    "#############################\n",
    "#\n",
    "# read the data\n",
    "#\n",
    "#############################\n",
    "\n",
    "def create_connection(database):\n",
    "    \"\"\" create a database connection to the SQLite database \n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(database)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None \n",
    "\n",
    "def read_the_shuffled_sentences_table():\n",
    "    db = './db/5pk.db'\n",
    "    conn = create_connection(db)\n",
    "    conn.text_factory = str\n",
    "    \n",
    "    #check if everything is really in the db table\n",
    "    return pd.read_sql_query(\"select * from tbl_Sentences;\", conn)\n",
    "\n",
    "def get_the_right_ordered_sentences():\n",
    "    return read_the_shuffled_sentences_table()\n",
    "\n",
    "def get_the_cleaned_sentences():\n",
    "    shuffle = read_the_shuffled_sentences_table()\n",
    "    \n",
    "    # frac = 1 means all the rows in random order\n",
    "    shuffle = shuffle.sample(frac=1)\n",
    "    shuffle.columns = shuffle.columns.str.strip()\n",
    "    #shuffle.shape\n",
    "    #shuffle.describe\n",
    "    \n",
    "    #make a copy not to change the original - dont need that here\n",
    "    #df = shuffle.copy()\n",
    "    \n",
    "    #replace some leading space\n",
    "    shuffle.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "    \n",
    "    #clean the \\r and \\n from sentences strings (replace it with whitespaces)\n",
    "    shuffle['Sentence'] = shuffle['Sentence'].map(lambda x: x.replace('\\r',' ').replace('\\n', ' '))\n",
    "    \n",
    "    return shuffle\n",
    "\n",
    "\n",
    "\n",
    "# save the shuffled-cleaned sentences to get everytime the same order\n",
    "# while dev status\n",
    "#\n",
    "#cleaned_sentences = get_the_cleaned_sentences()\n",
    "#\n",
    "#with open('cleaned_sentences.pickle', 'wb') as handle:\n",
    "#    pickle.dump(cleaned_sentences, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#\n",
    "# save the db inte a pickle in the right order\n",
    "#\n",
    "#\n",
    "#cleaned_sentences_right_order = get_the_right_ordered_sentences()\n",
    "#\n",
    "#with open('cleaned_sentences_right_order.pickle', 'wb') as handle:\n",
    "#    pickle.dump(cleaned_sentences_right_order, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "with open('cleaned_sentences_right_order.pickle', 'rb') as handle:\n",
    "    cleaned_sentences = pickle.load(handle) \n",
    "\n",
    "def get_tokens_lemma_and_pos(df):\n",
    "    nlp = spacy.load('de')\n",
    "    \n",
    "    tokens = []\n",
    "    lemma = []\n",
    "    pos = []\n",
    "    \n",
    "    for doc in nlp.pipe(df['Sentence'].astype('unicode').values, batch_size=50,\n",
    "                    n_threads=3):\n",
    "        if doc.is_parsed:\n",
    "            tokens.append([n.text for n in doc])\n",
    "            lemma.append([n.lemma_ for n in doc])\n",
    "            pos.append([n.pos_ for n in doc])\n",
    "        else:\n",
    "            # We want to make sure that the lists of parsed results have the\n",
    "            # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "            tokens.append(None)\n",
    "            lemma.append(None)\n",
    "            pos.append(None)\n",
    "        \n",
    "\n",
    "        \n",
    "    df['Sentence_tokens'] = tokens\n",
    "    df['Sentence_lemma'] = lemma\n",
    "    df['Sentence_pos'] = pos\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "#tagged_sentences = get_tokens_lemma_and_pos(cleaned_sentences)\n",
    "#tagged_sentences.info()\n",
    "\n",
    "\n",
    "######################################\n",
    "#\n",
    "# Count frequence of all the tokens - nouns, verbs, adj,....\n",
    "#\n",
    "######################################\n",
    "\n",
    "import gzip\n",
    "import humanfriendly\n",
    "import msgpack\n",
    "\n",
    "def remove_tensor(nlp, doc):\n",
    "    # taken from here:\n",
    "    # https://github.com/explosion/spaCy/issues/1600\n",
    "    payload = msgpack.loads(doc.to_bytes())\n",
    "    payload['tensor'] = None\n",
    "    thin_doc = doc(nlp.vocab)\n",
    "    print(5)\n",
    "    thin_doc.from_bytes(msgpack.dumps(payload))\n",
    "    return thin_doc\n",
    "\n",
    "\n",
    "def common_words(data, n):\n",
    "    \"\"\"\n",
    "    param data: pd.df with all the sentences\n",
    "    n: int number for the word frequency\n",
    "    \"\"\"\n",
    "    ###### - classifie the word types - ######\n",
    "    # init the spacy object for the german language\n",
    "    # http://spacy.io/models/de#/de_core_news_sm\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "    print(\"spacy load - de_core_news_sm - Done\")\n",
    "    \n",
    "    # get the text\n",
    "    #text = cleaned_sentences.to_string(columns=['Sentence'], header=False, index=False)\n",
    "    text = 'Der Zug, der am Montag gegen 15 Uhr aus Richtung Nordkorea.'\n",
    "    print(\"cleaned_sentences df into string - Done\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # text into the spacy instance\n",
    "    doc = nlp(text) # does not work memory error - spacy bug\n",
    "    \n",
    "    doc = remove_tensor(nlp, doc)\n",
    "\n",
    "\n",
    "    print(\"create the doc object withput the Tensor\") \n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        break\n",
    "    \n",
    "    print(\"text to spacy object - Done\") \n",
    "    \n",
    "    # all tokens that arent stop words or punctuations\n",
    "    words = ([token.text for token in doc if token.is_stop != True and token.is_punct != True])\n",
    "    print(\"words - Done\")\n",
    "    \n",
    "    # noun tokens that arent stop words or punctuations\n",
    "    nouns = [token.text for token in doc if token.is_stop != True and token.is_punct != True and token.pos_ == 'NOUN']\n",
    "\n",
    "    # verb tokens that arent stop words or punctuations\n",
    "    verbs = [token.text for token in doc if token.is_stop != True and token.is_punct != True and token.pos_ == 'VERB']\n",
    "\n",
    "    # adjective tokens that arent stop words or punctuations\n",
    "    adjs = [token.text for token in doc if token.is_stop != True and token.is_punct != True and token.pos_ == 'ADJ']\n",
    "    \n",
    "    # most common tokens\n",
    "    word_freq = Counter(words)\n",
    "    common_words = word_freq.most_common(n)\n",
    "    \n",
    "    # most common nouns\n",
    "    noun_freq = Counter(nouns)\n",
    "    common_nouns = word_freq.most_common(n)\n",
    "    \n",
    "    # most common verbs\n",
    "    verb_freq = Counter(verbs)\n",
    "    common_verbs = verb_freq.most_common(n)\n",
    "    \n",
    "    # most common adjectives\n",
    "    adj_freq = Counter(adjs)\n",
    "    common_adjs = adj_freq.most_common(n)\n",
    "\n",
    "    #print common words:\n",
    "    print(\"common words:\")\n",
    "    for w in common_words:\n",
    "        print(w)\n",
    "\n",
    "    #print common nouns:\n",
    "    print(\"common nouns:\")\n",
    "    for w in common_nouns:\n",
    "        print(w)\n",
    "    \n",
    "    #print common verbs:\n",
    "    print(\"common verbs:\")\n",
    "    for w in common_verbs:\n",
    "        print(w)\n",
    "    \n",
    "    #print common adjs:\n",
    "    print(\"common adjs:\")\n",
    "    for w in common_adjs:\n",
    "        print(w)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# list of all spacy POS tags:\n",
    "Tag\tPOS\tMorphology\tDescription\n",
    "$(\tPUNCT\tPunctType=brck\tother sentence-internal punctuation mark\n",
    "$,\tPUNCT\tPunctType=comm\tcomma\n",
    "$.\tPUNCT\tPunctType=peri\tsentence-final punctuation mark\n",
    "ADJA\tADJ\t\tadjective, attributive\n",
    "ADJD\tADJ\tVariant=short\tadjective, adverbial or predicative\n",
    "ADV\tADV\t\tadverb\n",
    "APPO\tADP\tAdpType=post\tpostposition\n",
    "APPR\tADP\tAdpType=prep\tpreposition; circumposition left\n",
    "APPRART\tADP\tAdpType=prep PronType=art\tpreposition with article\n",
    "APZR\tADP\tAdpType=circ\tcircumposition right\n",
    "ART\tDET\tPronType=art\tdefinite or indefinite article\n",
    "CARD\tNUM\tNumType=card\tcardinal number\n",
    "FM\tX\tForeign=yes\tforeign language material\n",
    "ITJ\tINTJ\t\tinterjection\n",
    "KOKOM\tCONJ\tConjType=comp\tcomparative conjunction\n",
    "KON\tCONJ\t\tcoordinate conjunction\n",
    "KOUI\tSCONJ\t\tsubordinate conjunction with \"zu\" and infinitive\n",
    "KOUS\tSCONJ\t\tsubordinate conjunction with sentence\n",
    "NE\tPROPN\t\tproper noun\n",
    "NNE\tPROPN\t\tproper noun\n",
    "NN\tNOUN\t\tnoun, singular or mass\n",
    "PAV\tADV\tPronType=dem\tpronominal adverb\n",
    "PROAV\tADV\tPronType=dem\tpronominal adverb\n",
    "PDAT\tDET\tPronType=dem\tattributive demonstrative pronoun\n",
    "PDS\tPRON\tPronType=dem\tsubstituting demonstrative pronoun\n",
    "PIAT\tDET\tPronType=ind neg tot\tattributive indefinite pronoun without determiner\n",
    "PIDAT\tDET\tAdjType=pdt PronType=ind neg tot\tattributive indefinite pronoun with determiner\n",
    "PIS\tPRON\tPronType=ind neg tot\tsubstituting indefinite pronoun\n",
    "PPER\tPRON\tPronType=prs\tnon-reflexive personal pronoun\n",
    "PPOSAT\tDET\tPoss=yes PronType=prs\tattributive possessive pronoun\n",
    "PPOSS\tPRON\tPronType=rel\tsubstituting possessive pronoun\n",
    "PRELAT\tDET\tPronType=rel\tattributive relative pronoun\n",
    "PRELS\tPRON\tPronType=rel\tsubstituting relative pronoun\n",
    "PRF\tPRON\tPronType=prs Reflex=yes\treflexive personal pronoun\n",
    "PTKA\tPART\t\tparticle with adjective or adverb\n",
    "PTKANT\tPART\tPartType=res\tanswer particle\n",
    "PTKNEG\tPART\tNegative=yes\tnegative particle\n",
    "PTKVZ\tPART\tPartType=vbp\tseparable verbal particle\n",
    "PTKZU\tPART\tPartType=inf\t\"zu\" before infinitive\n",
    "PWAT\tDET\tPronType=int\tattributive interrogative pronoun\n",
    "PWAV\tADV\tPronType=int\tadverbial interrogative or relative pronoun\n",
    "PWS\tPRON\tPronType=int\tsubstituting interrogative pronoun\n",
    "TRUNC\tX\tHyph=yes\tword remnant\n",
    "VAFIN\tAUX\tMood=ind VerbForm=fin\tfinite verb, auxiliary\n",
    "VAIMP\tAUX\tMood=imp VerbForm=fin\timperative, auxiliary\n",
    "VAINF\tAUX\tVerbForm=inf\tinfinitive, auxiliary\n",
    "VAPP\tAUX\tAspect=perf VerbForm=fin\tperfect participle, auxiliary\n",
    "VMFIN\tVERB\tMood=ind VerbForm=fin VerbType=mod\tfinite verb, modal\n",
    "VMINF\tVERB\tVerbForm=fin VerbType=mod\tinfinitive, modal\n",
    "VMPP\tVERB\tAspect=perf VerbForm=part VerbType=mod\tperfect participle, modal\n",
    "VVFIN\tVERB\tMood=ind VerbForm=fin\tfinite verb, full\n",
    "VVIMP\tVERB\tMood=imp VerbForm=fin\timperative, full\n",
    "VVINF\tVERB\tVerbForm=inf\tinfinitive, full\n",
    "VVIZU\tVERB\tVerbForm=inf\tinfinitive with \"zu\", full\n",
    "VVPP\tVERB\tAspect=perf VerbForm=part\tperfect participle, full\n",
    "XY\tX\t\tnon-word containing non-letter\n",
    "SP\tSPACE\t\tspace\n",
    "\"\"\"        \n",
    "\n",
    "#common = common_words(cleaned_sentences, 50)\n",
    "\n",
    "\n",
    "#############################################\n",
    "#\n",
    "#\n",
    "#    stastistic output - textstat\n",
    "#\n",
    "#\n",
    "#############################################\n",
    "from textstat.textstat import textstat as ts\n",
    "\n",
    "def stat_output(text):\n",
    "    # Python package to calculate statistics from text to determine readability,\n",
    "    # complexity and grade level of a particular corpus.\n",
    "    # https://github.com/shivam5992/textstat\n",
    "\n",
    "    #print(type(text))\n",
    "    print(\"cleaned_sentences df into string - Done\")\n",
    "\n",
    "    print(ts.flesch_reading_ease(text))\n",
    "    print(ts.smog_index(text))\n",
    "    print(ts.flesch_kincaid_grade(text))\n",
    "    print(ts.coleman_liau_index(text))\n",
    "    print(ts.automated_readability_index(text))\n",
    "    print(ts.dale_chall_readability_score(text))\n",
    "    print(ts.difficult_words(text))\n",
    "    print(ts.linsear_write_formula(text))\n",
    "    print(ts.gunning_fog(text))\n",
    "    print(ts.text_standard(text))\n",
    "\n",
    "\n",
    "text = cleaned_sentences.to_string(columns=['Sentence'], header=False, index=False)\n",
    "\n",
    "stat_output(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy load - Done\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a4873b1115c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spacy load - Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a4873b1115c4>\u001b[0m in \u001b[0;36mremove_tensor\u001b[0;34m(nlp, doc)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsgpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpayload\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tensor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mthin_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mthin_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Doc' is not defined"
     ]
    }
   ],
   "source": [
    "# test the remove tensor function\n",
    "\n",
    "import de_core_news_sm\n",
    "import spacy\n",
    "import gzip\n",
    "import humanfriendly\n",
    "import msgpack\n",
    "\n",
    "\n",
    "text = 'Der Zug, der am Montag gegen 15 Uhr aus Richtung Nordkorea.'\n",
    "\n",
    "\n",
    "def remove_tensor(nlp, doc):\n",
    "    # taken from here:\n",
    "    # https://github.com/explosion/spaCy/issues/1600\n",
    "    payload = msgpack.loads(doc.to_bytes())\n",
    "    payload['tensor'] = None\n",
    "    thin_doc = Doc(nlp.vocab)\n",
    "    print(5)\n",
    "    thin_doc.from_bytes(msgpack.dumps(payload))\n",
    "    return thin_doc\n",
    "\n",
    "\n",
    "\n",
    "nlp = de_core_news_sm.load()\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"spacy load - Done\")\n",
    "doc = remove_tensor(nlp, doc)\n",
    "\n",
    "\n",
    "print(\"create the doc object withput the Tensor - Done\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4th and 5th grade'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textstat.textstat import textstat\n",
    "# Python package to calculate statistics from text to determine readability,\n",
    "# complexity and grade level of a particular corpus.\n",
    "# https://github.com/shivam5992/textstat\n",
    "\n",
    "\n",
    "text = 'Der Zug, der am Montag gegen 15 Uhr aus Richtung Nordkorea.'\n",
    "\n",
    "textstat.flesch_reading_ease(text)\n",
    "textstat.smog_index(text)\n",
    "textstat.flesch_kincaid_grade(text)\n",
    "textstat.coleman_liau_index(text)\n",
    "textstat.automated_readability_index(text)\n",
    "textstat.dale_chall_readability_score(text)\n",
    "textstat.difficult_words(text)\n",
    "textstat.linsear_write_formula(text)\n",
    "textstat.gunning_fog(text)\n",
    "textstat.text_standard(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E001] No component 'parser' found in pipeline. Available names: []\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/textacy/corpus.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, texts, docs, metadatas)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mparser\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mget_pipe\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpipe_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E001] No component 'parser' found in pipeline. Available names: []\""
     ]
    }
   ],
   "source": [
    "%%time\n",
    "###########################\n",
    "#\n",
    "# try textacy\n",
    "# example taken from:\n",
    "# https://github.com/chartbeat-labs/textacy/issues/180\n",
    "#\n",
    "###########################\n",
    "\n",
    "\n",
    "import spacy\n",
    "import pickle\n",
    "import textacy\n",
    "import de_core_news_sm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# import the books\n",
    "with open('cleaned_sentences_right_order.pickle', 'rb') as handle:\n",
    "    cleaned_sentences = pickle.load(handle) \n",
    "\n",
    "# read the rawTexts\n",
    "rawTexts = cleaned_sentences.to_string(columns=['Sentence'], header=False, index=False)\n",
    "      \n",
    "# init the spacy NN\n",
    "nlp = spacy.load('de_core_news_sm', disable=['tagger', 'parser', 'ner'])\n",
    "\n",
    "# create the docs for the spacy/textacy corpus\n",
    "docs = [nlp(txt) for txt in rawTexts]\n",
    "\n",
    "# duration to process till here: 3min 34s\n",
    "\n",
    "# create the Corpus\n",
    "corpus = textacy.Corpus(lang=nlp, docs=docs)\n",
    "# duration to process till here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(type(common))\n",
    "print(common)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### have to check the following\n",
    "\n",
    "\n",
    "#Download the POS Tagger first\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "#<ConllCorpusReader in u'/home/user/Dokumente/Github/5pk/txt_edit'>\n",
    "root = '.'\n",
    "fileid = 'tiger.16012013.conll09'\n",
    "columntypes = ['ignore', 'words', 'ignore', 'ignore', 'pos']\n",
    "corp = nltk.corpus.ConllCorpusReader(root, fileid, columntypes, encoding='utf8')\n",
    "\n",
    "\n",
    "#load the tiger corpus to train the pos tagger later\n",
    "#corp = nltk.corpus.ConllCorpusReader('.', 'tiger_release_aug07.corrected.16012013.conll09',\n",
    "#                                     ['ignore', 'words', 'ignore', 'ignore', 'pos'],\n",
    "#                                     encoding='utf-8')\n",
    "\"\"\"\n",
    "print(corp)\n",
    "print(type(corp))\n",
    "print(corp.tagged_sents())\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "# explicitly make list, then LazySequence will traverse all items\n",
    "#https://stackoverflow.com/questions/39622121/nltk-perceptron-tagger-typeerror-lazysubsequence-object-does-not-support-ite\n",
    "tagged_sentences = [corp.tagged_sents()]\n",
    "i = math.floor(len(tagged_sentences)*0.2)\n",
    "testing_sentences = tagged_sentences[0:int(i)]\n",
    "training_sentences = tagged_sentences[int(i):]\n",
    "\n",
    "perceptron_tagger = nltk.tag.perceptron.PerceptronTagger(load=False)\n",
    "print(\"testtesttest\")\n",
    "print(training_sentences)\n",
    "print(len(training_sentences))\n",
    "perceptron_tagger.train(training_sentences)\n",
    "\n",
    "print(type(perceptron_tagger))\n",
    "\"\"\"\n",
    "\"\"\"the solution:\n",
    " https://stackoverflow.com/questions/39622121/nltk-perceptron-tagger-typeerror-lazysubsequence-object-does-not-support-ite\n",
    "\n",
    "Solution\n",
    "\n",
    "To work around the error, just explicitly create a list of the sentences from the nltk.corpus.brown package then random can shuffle the data properly.\n",
    "\n",
    "<code>\n",
    "    import nltk,math\n",
    "     explicitly make list, then LazySequence will traverse all items\n",
    "    tagged_sentences = [sentence for sentence in nltk.corpus.brown.tagged_sents(categories='news',tagset='universal')]\n",
    "    i = math.floor(len(tagged_sentences)*0.2)\n",
    "    testing_sentences = tagged_sentences[0:i]\n",
    "    training_sentences = tagged_sentences[i:]\n",
    "    perceptron_tagger = nltk.tag.perceptron.PerceptronTagger(load=False)\n",
    "    perceptron_tagger.train(training_sentences)\n",
    "\n",
    "</code>\n",
    "no error, yea!\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# set a split size: use 90% for training, 10% for testing\n",
    "split_perc = 0.1\n",
    "split_size = int(len(tagged_sents) * split_perc)\n",
    "train_sents, test_sents = tagged_sents[split_size:], tagged_sents[:split_size]\n",
    "\n",
    "\"\"\"\n",
    "#do a pos tagging for the test text\n",
    "#test = nltk.pos_tag(test)\n",
    "#print(test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for book in raw_books_schiller:\n",
    "    path = corpus_root + book[0]\n",
    "    html = open(path).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    raw = soup.get_text()\n",
    "    print(\"########### jetzt kommt der Type\")\n",
    "    print(type(raw))\n",
    "    raw = raw[237:]\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    \n",
    "    text = nltk.Text(tokens)\n",
    "\n",
    "    # Remove single-character tokens (mostly punctuation)\n",
    "    text = [word for word in text if len(word) > 1]\n",
    "    \n",
    "    # Remove numbers\n",
    "    words = [word for word in text if not word.isnumeric()]\n",
    "    \n",
    "    # Lowercase all words (default_stopwords are lowercase too)\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in all_stopwords]\n",
    "    \n",
    "    # Calculate frequency distribution\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    \n",
    "    # Output top 20 words\n",
    "    for word, frequency in fdist.most_common(20):\n",
    "        print(u'{};{}'.format(word, frequency))\n",
    "\n",
    "    \n",
    "#    fdist1 = FreqDist(words)\n",
    "#    fdist1.most_common(50)\n",
    "#    \n",
    "#    vocab = sorted(set(words))\n",
    "#    \n",
    "#    for word in vocab:\n",
    "#        print(word)\n",
    "\"\"\"   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-2-7361f1229c01>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-7361f1229c01>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    #tagged_sentences.tail()\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "df = tagged_sentences\n",
    "df2 = pd.DataFrame\n",
    "\n",
    "for Book_ID, new_df in df.groupby(level=0):\n",
    "   # print(new_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tagged_sentences.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "i = 0\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    print(filename)\n",
    "    f = open(path + '/' + filename)\n",
    "    soup = BeautifulSoup(f.read(), \"html5lib\")\n",
    "    i += 1\n",
    "    for i in soup.findAll(u\"\\u2018\"):\n",
    "        if i.text == u\"\\2018\" or u\"\\2019\" in i.text:\n",
    "            i.string = \"'\"\n",
    "    for i in soup.findAll(u\"\\u2019\"):\n",
    "        if i.text == u\"\\2018\" or u\"\\2019\" in i.text:\n",
    "            i.string = \"'\"\n",
    "    g = soup.get_text().encode('utf-8')\n",
    "    print(g)\n",
    "    \n",
    "    \n",
    "#for file in glob.glob('*.html'):\n",
    " #   detect()\n",
    " \n",
    " \"\"\"\n",
    "\n",
    "\"\"\"for book in raw_books_schiller:\n",
    "    print(book)\n",
    "    pcr = PlaintextCorpusReader(corpus_root + book[0],'.*')\n",
    "    pcr.fileids()\n",
    "    print(wordlist)\n",
    "    #num_chars = len(wordlist)\n",
    "    test = gutenberg.raw(wordlist)\n",
    "    num_words = gutenberg.words(wordlist)\n",
    "    num_sents = gutenberg.sents(wordlist)\n",
    "\n",
    "\n",
    "for book in raw_books_schiller:\n",
    "    num_chars = len(gutenberg.raw(book))\n",
    "    num_words = len(gutenberg.words(book))\n",
    "    num_sents = len(gutenberg.sents(book))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
